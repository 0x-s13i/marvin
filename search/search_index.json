{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"Sections","text":""},{"location":"api_reference/#components","title":"Components","text":"<ul> <li>AI Classifiers</li> <li>AI Functions</li> <li>AI Models</li> </ul>"},{"location":"api_reference/#settings","title":"Settings","text":"<ul> <li>Settings</li> </ul>"},{"location":"api_reference/#utilities","title":"Utilities","text":"<ul> <li><code>jinja</code></li> <li><code>logging</code></li> </ul>"},{"location":"api_reference/settings/","title":"settings","text":""},{"location":"api_reference/settings/#marvin.settings","title":"<code>marvin.settings</code>","text":"<p>Settings for configuring <code>marvin</code>.</p>"},{"location":"api_reference/settings/#marvin.settings--requirements","title":"Requirements","text":"<p>All you need to configure is your OpenAI API key.</p> <p>You can set this in <code>~/.marvin/.env</code> or as an environment variable on your system: <pre><code>MARVIN_OPENAI_API_KEY=sk-...\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.AssistantSettings","title":"<code>AssistantSettings</code>","text":"<p>Settings for the assistant API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default assistant model to use, defaults to <code>gpt-4-1106-preview</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.ImageSettings","title":"<code>ImageSettings</code>","text":"<p>Settings for OpenAI's image API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default image model to use, defaults to <code>dall-e-3</code>.</p> <code>size</code> <code>Literal['1024x1024', '1792x1024', '1024x1792']</code> <p>The default image size to use, defaults to <code>1024x1024</code>.</p> <code>response_format</code> <code>Literal['url', 'b64_json']</code> <p>The default response format to use, defaults to <code>url</code>.</p> <code>style</code> <code>Literal['vivid', 'natural']</code> <p>The default style to use, defaults to <code>vivid</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>Settings for the OpenAI API.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[SecretStr]</code> <p>Your OpenAI API key.</p> <code>organization</code> <code>Optional[str]</code> <p>Your OpenAI organization ID.</p> <code>chat</code> <code>ChatSettings</code> <p>Settings for the chat API.</p> <code>images</code> <code>ImageSettings</code> <p>Settings for the images API.</p> <code>audio</code> <code>AudioSettings</code> <p>Settings for the audio API.</p> <code>assistants</code> <code>AssistantSettings</code> <p>Settings for the assistants API.</p> Example <p>Set the OpenAI API key: <pre><code>import marvin\n\nmarvin.settings.openai.api_key = \"sk-...\"\n\nassert marvin.settings.openai.api_key.get_secret_value() == \"sk-...\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.Settings","title":"<code>Settings</code>","text":"<p>Settings for <code>marvin</code>.</p> <p>This is the main settings object for <code>marvin</code>.</p> <p>Attributes:</p> Name Type Description <code>openai</code> <code>OpenAISettings</code> <p>Settings for the OpenAI API.</p> <code>log_level</code> <code>str</code> <p>The log level to use, defaults to <code>DEBUG</code>.</p> Example <p>Set the log level to <code>INFO</code>: <pre><code>import marvin\n\nmarvin.settings.log_level = \"INFO\"\n\nassert marvin.settings.log_level == \"INFO\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.SpeechSettings","title":"<code>SpeechSettings</code>","text":"<p>Settings for OpenAI's speech API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default speech model to use, defaults to <code>tts-1-hd</code>.</p> <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The default voice to use, defaults to <code>alloy</code>.</p> <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac']</code> <p>The default response format to use, defaults to <code>mp3</code>.</p> <code>speed</code> <code>float</code> <p>The default speed to use, defaults to <code>1.0</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.temporary_settings","title":"<code>temporary_settings</code>","text":"<p>Temporarily override Marvin setting values, including nested settings objects.</p> <p>To override nested settings, use <code>__</code> to separate nested attribute names.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>The settings to override, including nested settings.</p> <code>{}</code> Example <p>Temporarily override the OpenAI API key: <pre><code>import marvin\nfrom marvin.settings import temporary_settings\n\n# Override top-level settings\nwith temporary_settings(log_level=\"INFO\"):\n    assert marvin.settings.log_level == \"INFO\"\nassert marvin.settings.log_level == \"DEBUG\"\n\n# Override nested settings\nwith temporary_settings(openai__api_key=\"new-api-key\"):\n    assert marvin.settings.openai.api_key.get_secret_value() == \"new-api-key\"\nassert marvin.settings.openai.api_key.get_secret_value().startswith(\"sk-\")\n</code></pre></p>"},{"location":"api_reference/components/ai_classifier/","title":"ai_classifier","text":""},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier","title":"<code>marvin.components.ai_classifier</code>","text":""},{"location":"api_reference/components/ai_function/","title":"ai_function","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function","title":"<code>marvin.components.ai_function</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction","title":"<code>AIFunction</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.amap","title":"<code>amap</code>  <code>async</code>","text":"<p>Map the AI function over a sequence of arguments. Runs concurrently.</p> <p>A <code>map</code> twin method is provided by the <code>expose_sync_method</code> decorator.</p> <p>You can use <code>map</code> or <code>amap</code> synchronously or asynchronously, respectively, regardless of whether the user function is synchronous or asynchronous.</p> <p>Arguments should be provided as if calling the function normally, but each argument must be a list. The function is called once for each item in the list, and the results are returned in a list.</p> <p>For example, fn.map([1, 2]) is equivalent to [fn(1), fn(2)].</p> <p>fn.map([1, 2], x=['a', 'b']) is equivalent to [fn(1, x='a'), fn(2, x='b')].</p>"},{"location":"api_reference/components/ai_model/","title":"ai_model","text":""},{"location":"api_reference/components/ai_model/#marvin.components.ai_model","title":"<code>marvin.components.ai_model</code>","text":""},{"location":"api_reference/utilities/asyncio/","title":"Asyncio","text":""},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio","title":"<code>marvin.utilities.asyncio</code>","text":"<p>Utilities for working with asyncio.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.ExposeSyncMethodsMixin","title":"<code>ExposeSyncMethodsMixin</code>","text":"<p>A mixin that can take functions decorated with <code>expose_sync_method</code> and automatically create synchronous versions.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.expose_sync_method","title":"<code>expose_sync_method</code>","text":"<p>Decorator that automatically exposes synchronous versions of async methods. Note it doesn't work with classmethods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the synchronous method.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>The decorated function.</p> Example <p>Basic usage: <pre><code>class MyClass(ExposeSyncMethodsMixin):\n\n    @expose_sync_method(\"my_method\")\n    async def my_method_async(self):\n        return 42\n\nmy_instance = MyClass()\nawait my_instance.my_method_async() # returns 42\nmy_instance.my_method()  # returns 42\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Runs a synchronous function in an asynchronous manner.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., T]</code> <p>The function to run.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The return value of the function.</p> Example <p>Basic usage: <pre><code>def my_sync_function(x: int) -&gt; int:\n    return x + 1\n\nawait run_async(my_sync_function, 1)\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_sync","title":"<code>run_sync</code>","text":"<p>Runs a coroutine from a synchronous context, either in the current event loop or in a new one if there is no event loop running. The coroutine will block until it is done. A thread will be spawned to run the event loop if necessary, which allows coroutines to run in environments like Jupyter notebooks where the event loop runs on the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>coroutine</code> <code>Coroutine[Any, Any, T]</code> <p>The coroutine to run.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The return value of the coroutine.</p> Example <p>Basic usage: <pre><code>async def my_async_function(x: int) -&gt; int:\n    return x + 1\n\nrun_sync(my_async_function(1))\n</code></pre></p>"},{"location":"api_reference/utilities/context/","title":"Context","text":""},{"location":"api_reference/utilities/context/#marvin.utilities.context","title":"<code>marvin.utilities.context</code>","text":"<p>Module for defining context utilities.</p>"},{"location":"api_reference/utilities/context/#marvin.utilities.context.ScopedContext","title":"<code>ScopedContext</code>","text":"<p><code>ScopedContext</code> provides a context management mechanism using <code>contextvars</code>.</p> <p>This class allows setting and retrieving key-value pairs in a scoped context, which is preserved across asynchronous tasks and threads within the same context.</p> <p>Attributes:</p> Name Type Description <code>_context_storage</code> <code>ContextVar</code> <p>A context variable to store the context data.</p> Example <p>Basic Usage of ScopedContext <pre><code>context = ScopedContext()\nwith context(key=\"value\"):\n    assert context.get(\"key\") == \"value\"\n# Outside the context, the value is no longer available.\nassert context.get(\"key\") is None\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/","title":"jinja","text":""},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja","title":"<code>marvin.utilities.jinja</code>","text":"<p>Module for Jinja utilities.</p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment","title":"<code>BaseEnvironment</code>","text":"<p>BaseEnvironment provides a configurable environment for rendering Jinja templates.</p> <p>This class encapsulates a Jinja environment with customizable global functions and template settings, allowing for flexible template rendering.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <p>The Jinja environment for template rendering.</p> <code>globals</code> <code>dict[str, Any]</code> <p>A dictionary of global functions and variables available in templates.</p> Example <p>Basic Usage of BaseEnvironment <pre><code>env = BaseEnvironment()\n\nrendered = env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered)  # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment.render","title":"<code>render</code>","text":"<p>Renders a given template <code>str</code> or <code>BaseTemplate</code> with provided context.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[str, Template]</code> <p>The template to be rendered.</p> required <code>**kwargs</code> <code>Any</code> <p>Context variables to be passed to the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The rendered template as a string.</p> Example <p>Basic Usage of <code>BaseEnvironment.render</code> <pre><code>from marvin.utilities.jinja import Environment as jinja_env\n\nrendered = jinja_env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered) # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.Transcript","title":"<code>Transcript</code>","text":"<p>Transcript is a model representing a conversation involving multiple roles.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the transcript.</p> <code>roles</code> <code>dict[str, str]</code> <p>The roles involved in the transcript.</p> <code>environment</code> <code>BaseEnvironment</code> <p>The jinja environment to use for rendering the transcript.</p> Example <p>Basic Usage of Transcript: <pre><code>from marvin.utilities.jinja import Transcript\n\ntranscript = Transcript(\n    content=\"system: Hello, there! user: Hello, yourself!\",\n    roles=[\"system\", \"user\"],\n)\nprint(transcript.render_to_messages())\n# [\n#   BaseMessage(content='system: Hello, there!', role='system'),\n#   BaseMessage(content='Hello, yourself!', role='user')\n# ]\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.split_text_by_tokens","title":"<code>split_text_by_tokens</code>","text":"<p>Splits a given text by a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be split.</p> required <code>split_tokens</code> <code>list[str]</code> <p>The tokens to split the text by.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>A list of tuples containing the token and the text following it.</p> Example <p>Basic Usage of <code>split_text_by_tokens</code> <pre><code>from marvin.utilities.jinja import split_text_by_tokens\n\ntext = \"Hello, World!\"\nsplit_tokens = [\"Hello\", \"World\"]\npairs = split_text_by_tokens(text, split_tokens)\nprint(pairs) # Output: [(\"Hello\", \", \"), (\"World\", \"!\")]\n</code></pre></p>"},{"location":"api_reference/utilities/logging/","title":"logging","text":""},{"location":"api_reference/utilities/logging/#marvin.utilities.logging","title":"<code>marvin.utilities.logging</code>","text":"<p>Module for logging utilities.</p>"},{"location":"api_reference/utilities/logging/#marvin.utilities.logging.get_logger","title":"<code>get_logger</code>  <code>cached</code>","text":"<p>Retrieves a logger with the given name, or the root logger if no name is given.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the logger to retrieve.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger with the given name, or the root logger if no name is given.</p> Example <p>Basic Usage of <code>get_logger</code> <pre><code>from marvin.utilities.logging import get_logger\n\nlogger = get_logger(\"marvin.test\")\nlogger.info(\"This is a test\") # Output: marvin.test: This is a test\n\ndebug_logger = get_logger(\"marvin.debug\")\ndebug_logger.debug_kv(\"TITLE\", \"log message\", \"green\")\n</code></pre></p>"},{"location":"api_reference/utilities/openai/","title":"Openai","text":""},{"location":"api_reference/utilities/openai/#marvin.utilities.openai","title":"<code>marvin.utilities.openai</code>","text":"<p>Module for working with OpenAI.</p>"},{"location":"api_reference/utilities/openai/#marvin.utilities.openai.get_client","title":"<code>get_client</code>","text":"<p>Retrieves an OpenAI client with the given api key and organization.</p> <p>Returns:</p> Type Description <code>AsyncClient</code> <p>The OpenAI client with the given api key and organization.</p> Example <p>Retrieving an OpenAI client <pre><code>from marvin.utilities.openai import get_client\n\nclient = get_client()\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/","title":"Pydantic","text":""},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic","title":"<code>marvin.utilities.pydantic</code>","text":"<p>Module for Pydantic utilities.</p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.cast_to_model","title":"<code>cast_to_model</code>","text":"<p>Casts a type or callable to a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>function_or_type</code> <code>Union[type, type[BaseModel], GenericAlias, Callable[..., Any]]</code> <p>The type or callable to cast to a Pydantic model.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the model to create.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the model to create.</p> <code>None</code> <code>field_name</code> <code>Optional[str]</code> <p>The name of the field to create.</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The Pydantic model created from the given type or callable.</p> Example <p>Basic Usage of <code>cast_to_model</code> <pre><code>from marvin.utilities.pydantic import cast_to_model\nfrom pydantic import BaseModel\n\ndef foo(bar: str) -&gt; str:\n    return bar\n\n# cast a function to a model\nmodel = cast_to_model(foo, name=\"Foo\")\nassert issubclass(model, BaseModel)\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.parse_as","title":"<code>parse_as</code>","text":"<p>Parse a given data structure as a Pydantic model via <code>TypeAdapter</code>.</p> <p>Read more about <code>TypeAdapter</code> here.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Any</code> <p>The type to parse the data as.</p> required <code>data</code> <code>Any</code> <p>The data to be parsed.</p> required <code>mode</code> <code>Literal['python', 'json', 'strings']</code> <p>The mode to use for parsing, either <code>python</code>, <code>json</code>, or <code>strings</code>. Defaults to <code>python</code>, where <code>data</code> should be a Python object (e.g. <code>dict</code>).</p> <code>'python'</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The parsed <code>data</code> as the given <code>type_</code>.</p> Example <p>Basic Usage of <code>parse_as</code> <pre><code>from marvin.utilities.pydantic import parse_as\nfrom pydantic import BaseModel\n\nclass ExampleModel(BaseModel):\n    name: str\n\n# parsing python objects\nparsed = parse_as(ExampleModel, {\"name\": \"Marvin\"})\nassert isinstance(parsed, ExampleModel)\nassert parsed.name == \"Marvin\"\n\n# parsing json strings\nparsed = parse_as(\n    list[ExampleModel],\n    '[{\"name\": \"Marvin\"}, {\"name\": \"Arthur\"}]',\n    mode=\"json\"\n)\nassert all(isinstance(item, ExampleModel) for item in parsed)\nassert parsed[0].name == \"Marvin\"\nassert parsed[1].name == \"Arthur\"\n\n# parsing raw strings\nparsed = parse_as(int, '123', mode=\"strings\")\nassert isinstance(parsed, int)\nassert parsed == 123\n</code></pre></p>"},{"location":"api_reference/utilities/slack/","title":"Slack","text":""},{"location":"api_reference/utilities/slack/#marvin.utilities.slack","title":"<code>marvin.utilities.slack</code>","text":"<p>Module for Slack-related utilities.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.edit_slack_message","title":"<code>edit_slack_message</code>  <code>async</code>","text":"<p>Edit an existing Slack message by appending new text or replacing it.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str</code> <p>The Slack channel ID.</p> required <code>ts</code> <code>str</code> <p>The timestamp of the message to edit.</p> required <code>new_text</code> <code>str</code> <p>The new text to append or replace in the message.</p> required <code>mode</code> <code>str</code> <p>The mode of text editing, 'append' (default) or 'replace'.</p> <code>'append'</code> <p>Returns:</p> Type Description <code>Response</code> <p>httpx.Response: The response from the Slack API.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.fetch_current_message_text","title":"<code>fetch_current_message_text</code>  <code>async</code>","text":"<p>Fetch the current text of a specific Slack message using its timestamp.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_thread_messages","title":"<code>get_thread_messages</code>  <code>async</code>","text":"<p>Get all messages from a slack thread.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_token","title":"<code>get_token</code>  <code>async</code>","text":"<p>Get the Slack bot token from the environment.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.search_slack_messages","title":"<code>search_slack_messages</code>  <code>async</code>","text":"<p>Search for messages in Slack workspace based on a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query.</p> required <code>max_messages</code> <code>int</code> <p>The maximum number of messages to retrieve.</p> <code>3</code> <code>channel</code> <code>str</code> <p>The specific channel to search in. Defaults to None, which searches all channels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of message contents and permalinks matching the query.</p>"},{"location":"api_reference/utilities/strings/","title":"Strings","text":""},{"location":"api_reference/utilities/strings/#marvin.utilities.strings","title":"<code>marvin.utilities.strings</code>","text":"<p>Module for string utilities.</p>"},{"location":"api_reference/utilities/tools/","title":"Tools","text":""},{"location":"api_reference/utilities/tools/#marvin.utilities.tools","title":"<code>marvin.utilities.tools</code>","text":"<p>Module for NLI tool utilities.</p>"},{"location":"community/","title":"The Marvin Community","text":"<p>We're thrilled you're interested in Marvin! Here, we're all about community. Marvin isn't just a tool, it's a platform for developers to collaborate, learn, and grow. We're driven by a shared passion for making Large Language Models (LLMs) more accessible and easier to use.</p>"},{"location":"community/#connect-on-discord-or-twitter","title":"Connect on Discord or Twitter","text":"<p>The heart of our community beats in our Discord server. It's a space where you can ask questions, share ideas, or just chat with like-minded developers. Don't be shy, join us on Discord or Twitter!</p>"},{"location":"community/#contributing-to-marvin","title":"Contributing to Marvin","text":"<p>Remember, Marvin is your tool. We want you to feel at home suggesting changes, requesting new features, and reporting bugs. Here's how you can contribute:</p> <ul> <li> <p>Issues: Encountered a bug? Have a suggestion? Open an issue in our GitHub repository. We appreciate your input!</p> </li> <li> <p>Pull Requests (PRs): Ready to contribute code? We welcome your pull requests! Not sure how to make a PR? Check out the GitHub guide.</p> </li> <li> <p>Discord Discussions: Have an idea but not quite ready to open an issue or PR? Discuss it with us on Discord first!</p> </li> </ul> <p>Remember, every contribution, no matter how small, is valuable. Don't worry about not being an expert or making mistakes. We're here to learn and grow together. Your input helps Marvin become better for everyone.</p> <p>Stay tuned for community events and more ways to get involved. Marvin is more than a project \u2013 it's a community. And we're excited for you to be a part of it!</p>"},{"location":"community/development_guide/","title":"Development Guide","text":""},{"location":"community/development_guide/#prerequisites","title":"Prerequisites","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"community/development_guide/#installation","title":"Installation","text":"<p>Clone a fork of the repository and install the dependencies: <pre><code>git clone https://github.com/youFancyUserYou/marvin.git\ncd marvin\n</code></pre></p> <p>Activate a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Install the dependencies in editable mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Install the pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"community/development_guide/#testing","title":"Testing","text":"<p>Run the tests that don't require an LLM: <pre><code>pytest -vv -m \"not llm\"\n</code></pre></p> <p>Run the LLM tests: <pre><code>pytest -vv -m \"llm\"\n</code></pre></p> <p>Run all tests: <pre><code>pytest -vv\n</code></pre></p>"},{"location":"community/development_guide/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Fork the repository and create a new branch: <pre><code>git checkout -b my-branch\n</code></pre></p> <p>Make your changes and commit them: <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre></p> <p>Push your changes to your fork: <pre><code>git push origin my-branch\n</code></pre></p> <p>Open a pull request on GitHub - ping us on Discord if you need help!</p>"},{"location":"community/feedback/","title":"Feedback \ud83d\udc99","text":"<p>We've been humbled and energized by the positive community response to Marvin.</p> <p>Tired: write comments to prompt copilot to write code.Wired: just write comments. it's cleaner :D https://t.co/FOA26lR9xN</p>\u2014 Andrej Karpathy (@karpathy) March 30, 2023 <p>Ok, I admit, I\u2019m getting more and more hyped about @AskMarvinAI. Some of these new functions are pretty legit looking. https://t.co/xhCCKp5kU5</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) April 21, 2023 <p>even the way ai_model uses ai_fn is chefs kiss, truely a craftsmanhttps://t.co/GcmWDeEVJSThey have spinner text.</p>\u2014 jason (@jxnlco) May 12, 2023 <p>The library is open-source: @AskMarvinAI, by @jlowin`@ai_model` is not the only magic Python decorator. There is also `@ai_fn` that makes any function an ambient LLM processor.https://t.co/ZXElyA0Ihp</p>\u2014 Jim Fan (@DrJimFan) May 14, 2023 <p>This is f**king cool. https://t.co/4PH6VAZPYo</p>\u2014 Pydantic (@pydantic) May 14, 2023 <p>Pretty slick\u2026 get Pydantic models from a string of Text. https://t.co/EnnQkzl4Ay</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) May 12, 2023 <p>Uhh how are people not talking about @AskMarvinAI more? @ai_fn is \ud83e\udd2f</p>\u2014 Rushabh Doshi (@radoshi) May 18, 2023"},{"location":"components/ai_classifier/","title":"AI Classifier","text":"<p>AI Classifiers are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_classifier</code> is a decorator that lets you use LLMs to choose options, tools, or classify input.    </p> <p>Example</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\nclass CustomerIntent(Enum):\n    \"\"\"Classifies the incoming users intent\"\"\"\n\n    SALES = 'SALES'\n    TECHNICAL_SUPPORT = 'TECHNICAL_SUPPORT'\n    BILLING_ACCOUNTS = 'BILLING_ACCOUNTS'\n    PRODUCT_INFORMATION = 'PRODUCT_INFORMATION'\n    RETURNS_REFUNDS = 'RETURNS_REFUNDS'\n    ORDER_STATUS = 'ORDER_STATUS'\n    ACCOUNT_CANCELLATION = 'ACCOUNT_CANCELLATION'\n    OPERATOR_CUSTOMER_SERVICE = 'OPERATOR_CUSTOMER_SERVICE'\n\n@ai_classifier\ndef classify_intent(text: str) -&gt; CustomerIntent:\n    '''Classifies the most likely intent from user input'''\n\nclassify_intent(\"I got double charged, can you help me out?\")\n</code></pre> <p>Result</p> <pre><code>&lt;CustomerIntent.RETURNS_REFUNDS: 'RETURNS_REFUNDS'&gt;\n</code></pre> <p>How it works</p> <p>     Marvin enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.   </p> <p>When to use</p> <p> <ol> <li> Best for classification tasks when no training data is available.      <li> Best for writing classifiers that need deduction or inference.      <p>OpenAI compatibility</p> <p> The technique that AI Classifiers use for speed and correctness is only available through the OpenAI API at this time. Therefore, AI Classifiers can only be used with OpenAI-compatible LLMs, including the Azure OpenAI service.   </p>"},{"location":"components/ai_classifier/#features","title":"Features","text":""},{"location":"components/ai_classifier/#bulletproof","title":"\ud83d\ude85 Bulletproof","text":"<p><code>ai_classifier</code> will always output one of the options you've given it</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/ai_classifier/#fast","title":"\ud83c\udfc3 Fast","text":"<p><code>ai_classifier</code> only asks your LLM to output one token, so it's blazing fast - on the order of ~200ms in testing.</p>"},{"location":"components/ai_classifier/#deterministic","title":"\ud83e\udee1 Deterministic","text":"<p><code>ai_classifier</code> will be deterministic so long as the underlying model and options does not change.</p>"},{"location":"components/ai_function/","title":"AI Function","text":"<p>AI Functions are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_fn</code> is a decorator that lets you use LLMs to generate outputs for Python functions without source code.   </p> <p>Example</p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef generate_recipe(ingredients: list[str]) -&gt; list[str]:\n    \"\"\"From a list of `ingredients`, generates a\n    complete instruction set to cook a recipe.\n    \"\"\"\n\n\ngenerate_recipe([\"lemon\", \"chicken\", \"olives\", \"coucous\"])\n</code></pre> <p>How it works</p> <p>     AI Functions take your function's name, description, signature, source code, type hints, and provided inputs to predict a likely output. By default, no source code is generated and any existing source code is not executed. The only runtime is the large language model.   </p> <p>When to use</p> <p> <ol> <li> Best for generative tasks: creation and summarization of text or data models.     <li> Best for writing functions that would otherwise be impossible to write.     <li> Great for data extraction, though: see AI Models."},{"location":"components/ai_function/#mapping","title":"Mapping","text":"<p>AI Functions can be mapped over sequences of arguments. Mapped functions run concurrently, which means they run practically in parallel (since they are IO-bound). Therefore, the map will complete as soon as the slowest function call finishes.</p> <p>To see how mapping works, consider this AI Function:</p> <pre><code>@ai_fn\ndef list_fruit(n: int, color: str = None) -&gt; list[str]:\n    \"\"\"\n    Returns a list of `n` fruit that all have the provided `color`\n    \"\"\"\n</code></pre> <p>Mapping is invoked by using the AI Function's <code>.map()</code> method. When mapping, you call the function as you normally would, except that each argument should be a list of items. The function will be called on each set of items (e.g. first with each argument's first item, then with each argument's second item, etc.). For example, this is the same as calling <code>list_fruit(2)</code> and <code>list_fruit(3)</code> concurrently:</p> <pre><code>list_fruit.map([2, 3])\n</code></pre> <pre><code>[['apple', 'banana'], ['apple', 'banana', 'orange']]\n</code></pre> <p>And this is the same as calling <code>list_fruit(2, color='orange')</code> and <code>list_fruit(3, color='red')</code> concurrently:</p> <pre><code>list_fruit.map([2, 3], color=[\"orange\", \"red\"])\n</code></pre> <pre><code>[['orange', 'orange'], ['apple', 'strawberry', 'cherry']]\n</code></pre>"},{"location":"components/ai_function/#features","title":"Features","text":""},{"location":"components/ai_function/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_fn</code> is fully type-safe. It works out of the box with Pydantic models in your function's parameters or return type.</p> <pre><code>from pydantic import BaseModel\nfrom marvin import ai_fn\n\n\nclass SyntheticCustomer(BaseModel):\n    age: int\n    location: str\n    purchase_history: list[str]\n\n\n@ai_fn\ndef generate_synthetic_customer_data(\n    n: int, locations: list[str], average_purchase_history_length: int\n) -&gt; list[SyntheticCustomer]:\n    \"\"\"Generates synthetic customer data based on the given parameters.\n    Parameters include the number of customers ('n'),\n    a list of potential locations, and the average length of a purchase history.\n    \"\"\"\n\n\ncustomers = generate_synthetic_customer_data(\n    5, [\"New York\", \"San Francisco\", \"Chicago\"], 3\n)\n</code></pre>"},{"location":"components/ai_function/#natural-language-api","title":"\ud83d\udde3\ufe0f Natural Language API","text":"<p>Marvin exposes an API to prompt an <code>ai_fn</code> with natural language. This lets you create a Language API for any function you can write down.</p> <pre><code>generate_synthetic_customer_data.prompt(\n    \"I need 10 profiles from rural US cities making between 3 and 7 purchases\"\n)\n</code></pre>"},{"location":"components/ai_function/#examples","title":"Examples","text":""},{"location":"components/ai_function/#customer-sentiment","title":"Customer Sentiment","text":"<p>Rapidly prototype natural language pipelines.</p> <p>     Use hallucination as a literal feature. Generate data that would be impossible     or prohibatively expensive to purchase as you rapidly protype NLP pipelines.    </p> <pre><code>@ai_fn\ndef analyze_customer_sentiment(reviews: list[str]) -&gt; dict:\n    \"\"\"\n    Returns an analysis of customer sentiment, including common\n    complaints, praises, and suggestions, from a list of product\n    reviews.\n    \"\"\"\n\n\n# analyze_customer_sentiment([\"I love this product!\", \"I hate this product!\"])\n</code></pre>"},{"location":"components/ai_function/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>General real fake data.</p> <p>     Use hallucination as a figurative feature. Use python or pydantic     to describe the data model you need, and generate realistic data on the fly      for sales demos.   </p> <pre><code>class FinancialReport(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef create_drip_email(n: int, market_conditions: str) -&gt; list[FinancialReport]:\n    \"\"\"\n    Generates `n` synthetic financial reports based on specified\n    `market_conditions` (e.g., 'recession', 'bull market', 'stagnant economy').\n    \"\"\"\n</code></pre> <pre><code>class IoTData(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef generate_synthetic_IoT_data(n: int, device_type: str) -&gt; list[IoTData]:\n    \"\"\"\n    Generates `n` synthetic data points mimicking those from a specified\n    `device_type` in an IoT system.\n    \"\"\"\n</code></pre>"},{"location":"components/ai_model/","title":"AI Model","text":"<p>AI Models are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p>A decorator that lets you extract structured data from unstructured text, documents, or instructions.</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre> <p>How it works</p> <p>AI Models use an LLM to extract, infer, or deduce data from the provided text. The data is parsed with Pydantic into the provided schema.</p> <p>When to use</p> <ul> <li>Best for extractive tasks: structuing of text or data models.</li> <li>Best for writing NLP pipelines that would otherwise be impossible to create.</li> <li>Good for model generation, though, see AI Function.</li> </ul>"},{"location":"components/ai_model/#creating-an-ai-model","title":"Creating an AI Model","text":"<p>AI Models are identical to Pydantic <code>BaseModels</code>, except that they can attempt to parse natural language to populate their fields. To build an effective AI Model, be as specific as possible with your field names, field descriptions, docstring, and instructions.</p> <p>To build a minimal AI model, decorate any standard Pydantic model, like this:</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    \"\"\"A representation of a US city and state\"\"\"\n\n    city: str = Field(description=\"The city's proper name\")\n    state: str = Field(description=\"The state's two-letter abbreviation (e.g. NY)\")\n\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/ai_model/#features","title":"Features","text":""},{"location":"components/ai_model/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_model</code> is fully type-safe. It works out of the box with Pydantic models.</p>"},{"location":"components/ai_model/#powered-by-deduction","title":"\ud83e\udde0 Powered by deduction","text":"<p><code>ai_model</code> gives your data model access to the knowledge and deductive power  of a Large Language Model. This means that your data model can infer answers to previous impossible tasks.</p> <pre><code>@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str\n    country: str\n    latitude: float\n    longitude: float\n\n\nLocation(\"He says he's from the windy city\")\n\n# Location(\n#   city='Chicago',\n#   state='Illinois',\n#   country='United States',\n#   latitude=41.8781,\n#   longitude=-87.6298\n# )\n</code></pre>"},{"location":"components/ai_model/#examples","title":"Examples","text":""},{"location":"components/ai_model/#resumes","title":"Resumes","text":"<pre><code>from typing import Optional\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\n@ai_model\nclass Resume(BaseModel):\n    first_name: str\n    last_name: str\n    phone_number: Optional[str]\n    email: str\n\n\nResume(\"Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io\").json(indent=2)\n\n# {\n# first_name: 'Ford',\n# last_name: 'Prefect',\n# email: 'ford@prefect.io',\n# phone: '(555) 5124-5242',\n# }\n</code></pre>"},{"location":"components/ai_model/#customer-service","title":"Customer Service","text":"<pre><code>import datetime\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\nclass Destination(pydantic.BaseModel):\n    start: datetime.date\n    end: datetime.date\n    city: Optional[str]\n    country: str\n    suggested_attractions: list[str]\n\n\n@ai_model\nclass Trip(pydantic.BaseModel):\n    trip_start: datetime.date\n    trip_end: datetime.date\n    trip_preferences: list[str]\n    destinations: List[Destination]\n\n\nTrip(\"\"\"\\\n    I've got all of June off, so hoping to spend the first\\\n    half of June in London and the second half in Rabat. I love \\\n    good food and going to museums.\n\"\"\").json(indent=2)\n\n# {\n#   \"trip_start\": \"2023-06-01\",\n#   \"trip_end\": \"2023-06-30\",\n#   \"trip_preferences\": [\n#     \"good food\",\n#     \"museums\"\n#   ],\n#   \"destinations\": [\n#     {\n#       \"start\": \"2023-06-01\",\n#       \"end\": \"2023-06-15\",\n#       \"city\": \"London\",\n#       \"country\": \"United Kingdom\",\n#       \"suggested_attractions\": [\n#         \"British Museum\",\n#         \"Tower of London\",\n#         \"Borough Market\"\n#       ]\n#     },\n#     {\n#       \"start\": \"2023-06-16\",\n#       \"end\": \"2023-06-30\",\n#       \"city\": \"Rabat\",\n#       \"country\": \"Morocco\",\n#       \"suggested_attractions\": [\n#         \"Kasbah des Oudaias\",\n#         \"Hassan Tower\",\n#         \"Rabat Archaeological Museum\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#electronic-health-records","title":"Electronic Health Records","text":"<pre><code>from datetime import date\nfrom typing import Optional, List\nfrom pydantic import BaseModel\n\n\nclass Patient(BaseModel):\n    name: str\n    age: int\n    is_smoker: bool\n\n\nclass Diagnosis(BaseModel):\n    condition: str\n    diagnosis_date: date\n    stage: Optional[str] = None\n    type: Optional[str] = None\n    histology: Optional[str] = None\n    complications: Optional[str] = None\n\n\nclass Treatment(BaseModel):\n    name: str\n    start_date: date\n    end_date: Optional[date] = None\n\n\nclass Medication(Treatment):\n    dose: Optional[str] = None\n\n\nclass BloodTest(BaseModel):\n    name: str\n    result: str\n    test_date: date\n\n\n@ai_model\nclass PatientData(BaseModel):\n    patient: Patient\n    diagnoses: List[Diagnosis]\n    treatments: List[Treatment]\n    blood_tests: List[BloodTest]\n\n\nPatientData(\"\"\"\\\nMs. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nUnfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019.\nMs. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nMs. Lee was initially diagnosed with stage I hypertension on 06-01-2018.\nMs. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018.\nMs. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018.\nMs. Lee's most recent A1C level was 8.5% on 06-15-2020.\nMs. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019.\nMs. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019.\nMs. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"patient\": {\n#     \"name\": \"Ms. Lee\",\n#     \"age\": 45,\n#     \"is_smoker\": false\n#   },\n#   \"diagnoses\": [\n#     {\n#       \"condition\": \"Type 2 diabetes mellitus\",\n#       \"diagnosis_date\": \"2018-06-01\",\n#       \"stage\": \"I\",\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     },\n#     {\n#       \"condition\": \"Diabetic retinopathy\",\n#       \"diagnosis_date\": \"2019-09-01\",\n#       \"stage\": null,\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     }\n#   ],\n#   \"treatments\": [\n#     {\n#       \"name\": \"Metformin\",\n#       \"start_date\": \"2018-06-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Lisinopril\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Atorvastatin\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     }\n#   ],\n#   \"blood_tests\": [\n#     {\n#       \"name\": \"A1C\",\n#       \"result\": \"8.5%\",\n#       \"test_date\": \"2020-06-15\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2018-06-01\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2019-09-01\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#text-to-sql","title":"Text to SQL","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom django.db.models import Q\n\n\nclass DjangoLookup(BaseModel):\n    field: Literal[*django_fields]\n    lookup: Literal[*django_lookups] = pydantic.Field(description=\"e.g. __iregex\")\n    value: Any\n\n\n@ai_model\nclass DjangoQuery(BaseModel):\n    \"\"\"A model representing a Django ORM query\"\"\"\n\n    lookups: List[DjangoLookup]\n\n    def to_q(self) -&gt; Q:\n        q = Q()\n        for lookup in self.lookups:\n            q &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})\n        return q\n\n\nDjangoQuery(\"\"\"\\\n    All users who joined more than two months ago but\\\n    haven't made a purchase in the last 30 days\"\"\").to_q()\n\n# &lt;Q: (AND:\n#     ('date_joined__lte', '2023-03-11'),\n#     ('last_purchase_date__isnull', False),\n#     ('last_purchase_date__lte', '2023-04-11'))&gt;\n</code></pre>"},{"location":"components/ai_model/#financial-reports","title":"Financial Reports","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\n\n\n@ai_model\nclass CapTable(BaseModel):\n    total_authorized_shares: int\n    total_common_share: int\n    total_common_shares_outstanding: Optional[int]\n    total_preferred_shares: int\n    conversion_price_multiple: int = 1\n\n\nCapTable(\"\"\"\\\n    In the cap table for Charter, the total authorized shares amount to 13,250,000. \n    The total number of common shares stands at 10,000,000 as specified in Article Fourth, \n    clause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not \n    available at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned \n    in Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is \n    set at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is \n    derived from the Term Sheet.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"total_authorized_shares\": 13250000,\n#   \"total_common_share\": 10000000,\n#   \"total_common_shares_outstanding\": null,\n#   \"total_preferred_shares\": 3250000,\n#   \"conversion_price_multiple\": 3\n# }\n</code></pre>"},{"location":"components/ai_model/#meeting-notes","title":"Meeting Notes","text":"<pre><code>import datetime\nfrom typing import List\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\nfrom marvin import ai_model\n\n\nclass ActionItem(BaseModel):\n    responsible: str\n    description: str\n    deadline: Optional[datetime.datetime]\n    time_sensitivity: Literal[\"low\", \"medium\", \"high\"]\n\n\n@ai_model\nclass Conversation(BaseModel):\n    \"\"\"A class representing a team conversation\"\"\"\n\n    participants: List[str]\n    action_items: List[ActionItem]\n\n\nConversation(\"\"\"\n    Adam: Hey Jeremiah can you approve my PR? I requested you to review it.\n    Jeremiah: Yeah sure, when do you need it done by?\n    Adam: By this Friday at the latest, we need to ship it by end of week.\n    Jeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.\n    Nate: Jeremiah we can meet today to chat.\n    Jeremiah: Okay, I'll book something for today.\n\"\"\").json(indent=2)\n\n# {\n#   \"participants\": [\n#     \"Adam\",\n#     \"Jeremiah\",\n#     \"Nate\"\n#   ],\n#   \"action_items\": [\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Approve Adam's PR\",\n#       \"deadline\": \"2023-05-12T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     },\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Book a meeting with Nate\",\n#       \"deadline\": \"2023-05-11T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/overview/","title":"AI Components","text":"<p>Marvin introduces a number of components that can become the building blocks of AI-powered software. When using any of Marvin's components, you have  a few options to customize the client and completion behavior.</p> How to: Customize your client <p>When using any of Marvin's components, you have two options to customize the client used. </p> Pass your clientUse runtime settings <p>You can explicitly pass a client to a component, like <code>ai_fn</code>. </p> <pre><code>client = OpenAI(api_key = 'sk-my-key')\n\n@ai_fn(client = client)\ndef my_ai_fn():\n    \"\"\"...\"\"\"\n</code></pre> <p>You can persist your client details into Marvin's settings, by setting an <code>ENV</code> variable  or by setting it at runtime. </p> <pre><code>from marvin import settings\nsettings.openai.api_key = 'sk-my-key'\n\n@ai_fn\ndef my_ai_fn():\n    \"\"\"...\"\"\"\n</code></pre> How to: Customize your completion <p>When using any of Marvin's components, you have two options to customize the client used. </p> Pass completion keywordsUse runtime settings <p>You can explicitly pass a completion keywords to a component, like <code>ai_fn</code>. </p> <pre><code>client = OpenAI(api_key = 'sk-my-key')\n\n@ai_fn(client = client, model = 'gpt-3.5-turbo')\ndef my_ai_fn():\n    \"\"\"...\"\"\"\n</code></pre> <p>You can persist your client details into Marvin's settings, by setting an <code>ENV</code> variable  or by setting it at runtime. </p> <pre><code>from marvin import settings\nsettings.openai.chat.completions.model = 'gpt-3.5-turbo'\n\n@ai_fn\ndef my_ai_fn():\n    \"\"\"...\"\"\"\n</code></pre>"},{"location":"components/overview/#components","title":"Components","text":""},{"location":"components/overview/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, built on Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data and entity extraction.</p> <p>Example</p> As a decoratorAs a function <p><code>ai_model</code> can decorate pydantic models to give them parsing powers. <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\n@ai_model(client = client)\nclass Location(BaseModel):\n    city: str\n    state_abbreviation: str = Field(\n        ..., \n        description=\"The two-letter state abbreviation\"\n    )\n\n\nLocation(\"The Big Apple\")\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>Location.as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code> {\n    \"tools\": [\n        {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"FormatResponse\",\n            \"parameters\": {\n            \"$defs\": {\n                \"Location\": {\n                \"properties\": {\n                    \"city\": {\n                    \"title\": \"City\",\n                    \"type\": \"string\"\n                    },\n                    \"state_abbreviation\": {\n                    \"description\": \"The two-letter state abbreviation\",\n                    \"title\": \"State Abbreviation\",\n                    \"type\": \"string\"\n                    }\n                },\n                \"required\": [\n                    \"city\",\n                    \"state_abbreviation\"\n                ],\n                \"title\": \"Location\",\n                \"type\": \"object\"\n                }\n            },\n            \"properties\": {\n                \"data\": {\n                \"allOf\": [\n                    {\n                    \"$ref\": \"#/$defs/Location\"\n                    }\n                ],\n                \"description\": \"The data to format.\"\n                }\n            },\n            \"required\": [\n                \"data\"\n            ],\n            \"type\": \"object\"\n            }\n        }\n        }\n    ],\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\n        \"name\": \"FormatResponse\"\n        }\n    },\n    \"messages\": [\n        {\n        \"content\": \"The user will provide context as text that you need to parse\n        into a structured form. To validate your response, you must call the \n        `FormatResponse` function. Use the provided text to extract or infer any \n        parameters needed by `FormatResponse`, including any missing data.\",\n        \"role\": \"system\"\n        },\n        {\n        \"content\": \"The text to parse: The Big Apple\",\n        \"role\": \"user\"\n        }\n    ]\n}\n</code></pre> <p><code>ai_model</code> can cast unstructured data to any <code>type</code> (or <code>GenericAlias</code>). <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\nclass Location(BaseModel):\n    city: str\n    state_abbreviation: str = Field(\n        ..., \n        description=\"The two-letter state abbreviation\"\n    )\n\nai_model(Location, client = client)(\"The Big Apple\")\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>ai_model(Location, client = client)(\"The Big Apple\").as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>    {\n    \"tools\": [\n        {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"FormatResponse\",\n            \"parameters\": {\n            \"$defs\": {\n                \"Location\": {\n                \"properties\": {\n                    \"city\": {\n                    \"title\": \"City\",\n                    \"type\": \"string\"\n                    },\n                    \"state_abbreviation\": {\n                    \"description\": \"The two-letter state abbreviation\",\n                    \"title\": \"State Abbreviation\",\n                    \"type\": \"string\"\n                    }\n                },\n                \"required\": [\n                    \"city\",\n                    \"state_abbreviation\"\n                ],\n                \"title\": \"Location\",\n                \"type\": \"object\"\n                }\n            },\n            \"properties\": {\n                \"data\": {\n                \"allOf\": [\n                    {\n                    \"$ref\": \"#/$defs/Location\"\n                    }\n                ],\n                \"description\": \"The data to format.\"\n                }\n            },\n            \"required\": [\n                \"data\"\n            ],\n            \"type\": \"object\"\n            }\n        }\n        }\n    ],\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\n        \"name\": \"FormatResponse\"\n        }\n    },\n    \"messages\": [\n        {\n        \"content\": \"The user will provide context as text that you need to parse\n        into a structured form. To validate your response, you must call the \n        `FormatResponse` function. Use the provided text to extract or infer any \n        parameters needed by `FormatResponse`, including any missing data.\",\n        \"role\": \"system\"\n        },\n        {\n        \"content\": \"The text to parse: The Big Apple\",\n        \"role\": \"user\"\n        }\n    ]\n}\n</code></pre> <p>Result</p> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/overview/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. Given user input, each classifier uses a clever logit bias trick to force an LLM to deductively choose the best option. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <p>Example</p> As a decoratorAs a function <p><code>ai_classifier</code> can decorate python functions whose return annotation is an <code>Enum</code> or <code>Literal</code>. The prompt is tuned for classification tasks,  and uses a form of <code>constrained sampling</code> to make guarantee a fast valid choice. <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n@ai_classifier(client = client)\ndef classify_intent(text: str) -&gt; AppRoute:\n    '''Classifies user's intent into most useful route'''\n\nclassify_intent(\"update my name\")\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>classify_intent.as_prompt(\"update my name\").serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. The prompt you send is fully customizable.  <pre><code>{\n\"logit_bias\": {\n    \"15\": 100.0,\n    \"16\": 100.0,\n    \"17\": 100.0,\n    \"18\": 100.0,\n    \"19\": 100.0,\n    \"20\": 100.0,\n    \"21\": 100.0,\n    \"22\": 100.0,\n    \"23\": 100.0\n},\n\"max_tokens\": 1,\n\"messages\": [\n    {\n    \"content\": \"## Expert Classifier\\n\\n        **Objective**: You are an expert classifier that always chooses correctly.\\n\\n        ### Context\\n        Classifies user's intent into most useful route\\n        \\n        ### Response Format\\n        You must classify the user provided data into one of the following classes:\\n        - Class 0 (value: USER_PROFILE)\\n        - Class 1 (value: SEARCH)\\n        - Class 2 (value: NOTIFICATIONS)\\n        - Class 3 (value: SETTINGS)\\n        - Class 4 (value: HELP)\\n        - Class 5 (value: CHAT)\\n        - Class 6 (value: DOCS)\\n        - Class 7 (value: PROJECTS)\\n        - Class 8 (value: WORKSPACES)\",\n    \"role\": \"system\"\n    },\n    {\n    \"content\": \"### Data\\n        The user provided the following data:                                                                                                                     \\n        - text: update my name\",\n    \"role\": \"assistant\"\n    },\n    {\n    \"content\": \"The most likely class label for the data and context provided above is Class\\\"\",\n    \"role\": \"assistant\"\n    }\n],\n\"temperature\": 0.0\n}\n</code></pre></p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\ndef classify_intent(text: str) -&gt; AppRoute:\n    '''Classifies user's intent into most useful route'''\n\nai_classifier(classify_intent, client = client)(\"update my name\")\n</code></pre> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>ai_classifier(classify_intent, client = client).as_prompt(\"update my name\").serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. The prompt you send is fully customizable.  <pre><code>{\n    \"logit_bias\": {\n        \"15\": 100.0,\n        \"16\": 100.0,\n        \"17\": 100.0,\n        \"18\": 100.0,\n        \"19\": 100.0,\n        \"20\": 100.0,\n        \"21\": 100.0,\n        \"22\": 100.0,\n        \"23\": 100.0\n    },\n    \"max_tokens\": 1,\n    \"messages\": [\n        {\n        \"content\": \"## Expert Classifier\\n\\n        **Objective**: You are an expert classifier that always chooses correctly.\\n\\n        ### Context\\n        Classifies user's intent into most useful route\\n        \\n        ### Response Format\\n        You must classify the user provided data into one of the following classes:\\n        - Class 0 (value: USER_PROFILE)\\n        - Class 1 (value: SEARCH)\\n        - Class 2 (value: NOTIFICATIONS)\\n        - Class 3 (value: SETTINGS)\\n        - Class 4 (value: HELP)\\n        - Class 5 (value: CHAT)\\n        - Class 6 (value: DOCS)\\n        - Class 7 (value: PROJECTS)\\n        - Class 8 (value: WORKSPACES)\",\n        \"role\": \"system\"\n        },\n        {\n        \"content\": \"### Data\\n        The user provided the following data:                                                                                                                     \\n        - text: update my name\",\n        \"role\": \"assistant\"\n        },\n        {\n        \"content\": \"The most likely class label for the data and context provided above is Class\\\"\",\n        \"role\": \"assistant\"\n        }\n    ],\n    \"temperature\": 0.0\n}\n</code></pre></p> <p>Result</p> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/overview/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis.</p> <p>Example</p> As a decoratorAs a function <p><code>ai_fn</code> can decorate python functions to evlaute them using a Large Language Model. <pre><code>from marvin import ai_fn\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\n@ai_fn(client=client)\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>sentiment_list.as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>{\n\"tools\": [\n    {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"FormatResponse\",\n        \"parameters\": {\n        \"properties\": {\n            \"data\": {\n            \"description\": \"The data to format.\",\n            \"items\": {\n                \"type\": \"number\"\n            },\n            \"title\": \"Data\",\n            \"type\": \"array\"\n            }\n        },\n        \"required\": [\n            \"data\"\n        ],\n        \"type\": \"object\"\n        }\n    }\n    }\n],\n\"tool_choice\": {\n    \"type\": \"function\",\n    \"function\": {\n    \"name\": \"FormatResponse\"\n    }\n},\n\"messages\": [\n    {\n    \"content\": \"Your job is to generate likely outputs for a Python function with the\\n        following signature and docstring:\\n\\n        \\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\\n    \\\"\\\"\\\"\\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\\n    -1 (negative) indicating their respective sentiment scores.\\n    \\\"\\\"\\\"\\n\\n\\n        The user will provide function inputs (if any) and you must respond with\\n        the most likely result.\",\n    \"role\": \"system\"\n    },\n    {\n    \"content\": \"The function was called with the following inputs:\\n        - texts: ['That was surprisingly easy!', 'Oh no, not again.']\\n\\n        What is its output?\",\n    \"role\": \"user\"\n    }\n]\n}\n</code></pre> <p><code>ai_fn</code> can be used as a utility function to evaluate python functions using a Large Language Model. <pre><code>from marvin import ai_fn\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nai_fn(sentiment_list, client=client)(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>ai_fn(sentiment_list, client = client)([\n    \"That was surprisingly easy!\",\n    \"Oh no, not again.\",\n]).as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>   {\n        \"tools\": [\n            {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"FormatResponse\",\n                \"parameters\": {\n                \"properties\": {\n                    \"data\": {\n                    \"description\": \"The data to format.\",\n                    \"items\": {\n                        \"type\": \"number\"\n                    },\n                    \"title\": \"Data\",\n                    \"type\": \"array\"\n                    }\n                },\n                \"required\": [\n                    \"data\"\n                ],\n                \"type\": \"object\"\n                }\n            }\n            }\n        ],\n        \"tool_choice\": {\n            \"type\": \"function\",\n            \"function\": {\n            \"name\": \"FormatResponse\"\n            }\n        },\n        \"messages\": [\n            {\n            \"content\": \"Your job is to generate likely outputs for a Python function with the\\n        following signature and docstring:\\n\\n        \\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\\n    \\\"\\\"\\\"\\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\\n    -1 (negative) indicating their respective sentiment scores.\\n    \\\"\\\"\\\"\\n\\n\\n        The user will provide function inputs (if any) and you must respond with\\n        the most likely result.\",\n            \"role\": \"system\"\n            },\n            {\n            \"content\": \"The function was called with the following inputs:\\n        - texts: ['That was surprisingly easy!', 'Oh no, not again.']\\n\\n        What is its output?\",\n            \"role\": \"user\"\n            }\n        ]\n        }\n</code></pre> <p>Result</p> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"components/overview/#utilities","title":"Utilities","text":"<p>Every Marvin component makes use of two serialization conveniences, which you're free to use  if you want to create your own opinionated components.</p>"},{"location":"components/overview/#prompt-functions","title":"Prompt Functions","text":"<p>Prompt Functions are responsible for taking a Python function and serializing it to a payload for a Large Language Model API to understand. It does not call or require an LLM provider. It's essentially a type-safe Jinja template that makes the locals of your function available for template formatting. </p> <p>Example</p> As a decoratorAs a function <p><code>prompt_fn</code> can decorate python functions to serialize them to a payload which them using a Large Language Model. It's especially useful if you want to use your own custom LLM but enjoy the ergonomics of Marvin. <pre><code>from marvin import prompt_fn\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"\n    Generates a list of {{ n }} {{ color }} fruits.\n    \"\"\"\n\n\nlist_fruits(3, 'blue')\n</code></pre></p> Result <p>It generates the raw payload that can be sent to an LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>{\n\"tools\": [\n    {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"FormatResponse\",\n        \"parameters\": {\n        \"properties\": {\n            \"data\": {\n            \"description\": \"The data to format.\",\n            \"items\": {\n                \"type\": \"string\"\n            },\n            \"title\": \"Data\",\n            \"type\": \"array\"\n            }\n        },\n        \"required\": [\n            \"data\"\n        ],\n        \"type\": \"object\"\n        }\n    }\n    }\n],\n\"tool_choice\": {\n    \"type\": \"function\",\n    \"function\": {\n    \"name\": \"FormatResponse\"\n    }\n},\n\"messages\": [\n    {\n    \"content\": \"Generate a list of 3 blue fruits.\",\n    \"role\": \"system\"\n    }\n]\n}\n</code></pre> <p><code>prompt_fn</code> can be used as a utility function to seraizlie python functions to prompts for a Large Language Model. It's especially useful if you want to use your own custom LLM but enjoy the ergonomics of Marvin. <pre><code>from marvin import prompt_fn\n\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"\n    Generates a list of {{ n }} {{ color }} fruits.\n    \"\"\"\n\n\nprompt_fn(list_fruits)(3, 'blue')\n</code></pre></p> Result <p>It generates the raw payload that can be sent to an LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>{\n\"tools\": [\n    {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"FormatResponse\",\n        \"parameters\": {\n        \"properties\": {\n            \"data\": {\n            \"description\": \"The data to format.\",\n            \"items\": {\n                \"type\": \"string\"\n            },\n            \"title\": \"Data\",\n            \"type\": \"array\"\n            }\n        },\n        \"required\": [\n            \"data\"\n        ],\n        \"type\": \"object\"\n        }\n    }\n    }\n],\n\"tool_choice\": {\n    \"type\": \"function\",\n    \"function\": {\n    \"name\": \"FormatResponse\"\n    }\n},\n\"messages\": [\n    {\n    \"content\": \"Generate a list of 3 blue fruits.\",\n    \"role\": \"system\"\n    }\n]\n}\n</code></pre>"},{"location":"configuration/settings/","title":"Settings","text":"<p>Marvin makes use of Pydantic's <code>BaseSettings</code> to configure, load, and change behavior.</p>"},{"location":"configuration/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings are configurable via environment variables like <code>MARVIN_&lt;setting name&gt;</code>.</p> <p>Setting Environment Variables</p> <p>For example, in an <code>.env</code> file or in your shell config file you might have: <pre><code>MARVIN_LOG_LEVEL=DEBUG\nMARVIN_LLM_MODEL=gpt-4\nMARVIN_LLM_TEMPERATURE=0\nMARVIN_OPENAI_API_KEY='sk-my-api-key'\n</code></pre> Settings these values will let you avoid setting an API key every time. </p>"},{"location":"configuration/settings/#runtime-settings","title":"Runtime Settings","text":"<p>A runtime settings object is accessible via <code>marvin.settings</code> and can be used to access or update settings throughout the package.</p> <p>Mutating settings at runtime</p> <p>For example, to access or change the LLM model used by Marvin at runtime: <pre><code>import marvin\n\nmarvin.settings.llm_model # 'gpt-4'\n\nmarvin.settings.llm_model = 'gpt-3.5-turbo'\n\nmarvin.settings.llm_model # 'gpt-3.5-turbo'\n</code></pre></p>"},{"location":"examples/classification_api/","title":"Basic Classifier API","text":"<p>With Marvin, you can easily build a production-grade application or data pipeline to classify data from unstructured text.  In this example, we'll show how to </p> <ul> <li>Write an AI-powered Function.</li> <li>Build a production-ready API.</li> </ul>"},{"location":"examples/classification_api/#writing-an-ai-powered-function","title":"Writing an AI-powered Function.","text":"<p>Example</p> <p>Marvin translates your Python code into English, passes that to an Large Language Model,  and parses its response. It uses AI to evaluate your function, no code required.</p> <p>Let's build a function that classifies a paragraph of text into predefined categories.  This is typically a pretty daunting task for a machine learning savant, much less your average engineer.  Marvin lets you accomplish this by writing code the way you normally would, no PhD required.</p> <p>We'll simply write a Python function, tell it that we expect a <code>text</code> input and that it'll  output a <code>str</code>, or string. With Marvin, we'll use <code>ai_fn</code> and decorate this function.  When we do, this function will use AI to get its answer.</p> <p><pre><code>from marvin import ai_fn, settings\nfrom typing import Literal\n\nsettings.openai.api_key = 'API_KEY' \n\n@ai_fn\ndef classify_text(text: str) -&gt; Literal['sports', 'politics', 'technology']:\n    '''\n        Correctly classifies the passed `text` into one of the predefined categories. \n    '''\n</code></pre> This function can now be run! When we test it out, we get great results.</p> Results <pre><code>classify_text('The Lakers won the game last night')\n\n# returns Category.SPORTS\n</code></pre>"},{"location":"examples/classification_api/#build-a-production-ready-api","title":"Build a production-ready API.","text":"<p>In the following example, we will demonstrate how to deploy the AI function we just created as an API using FastAPI. FastAPI is a powerful tool that allows us to easily turn our AI function into a fully-fledged API. This API can then be used by anyone to send a POST request to our <code>/classify_text/</code> endpoint and get the classified category they need. Let's see how this can be done.</p> <p>Example</p> <p>Now that we have our AI function, let's deploy it as an API using FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, settings\nfrom typing import Literal\n\nsettings.openai.api_key = 'API_KEY' \n\napp = FastAPI()\n\nsettings.openai.api_key = 'API_KEY'\n\n@app.post(\"/classify_text/\")\n@ai_fn\ndef classify_text(text: str) -&gt; Literal['sports', 'politics', 'technology']:\n    '''\n        Correctly classifies the passed `text` into one of the predefined categories. \n    '''\n</code></pre> <p>With just a few lines of code, we've turned our AI function into a fully-fledged API. Now, anyone can send a POST request to our <code>/classify_text/</code> endpoint and get the classified category they need.</p> API Deployment <p><pre><code>import uvicorn\nimport asyncio\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nasyncio.run(server.serve())\n</code></pre> Now, you can navigate to localhost:8000/docs to interact with your API.</p> Making Requests <p><pre><code>import requests\n\ndata = {\"text\": \"The Lakers won the game last night\"}\nresponse = requests.post(\"http://localhost:8000/classify_text/\", json=data)\nprint(response.json())\n\n# returns 'SPORTS'\n</code></pre> This will send a POST request to the <code>/classify_text/</code> endpoint with the provided text and print the response.</p>"},{"location":"examples/deduplication/","title":"Entity Deduplication","text":"<p>What is entity deduplication?</p> <p>How many distinct cities are mentioned in the following text:</p> <p>Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco. </p> <p>We know it's three, but getting software to deduplicate these entities is surprisingly hard. </p> <p>How can we turn it into something cleaner like:</p> <pre><code>[\n    City(text='Chicago', inferred_city='Chicago'),\n    City(text='The Windy City', inferred_city='Chicago'),\n    City(text='New York City', inferred_city='New York City'),\n    City(text='The Big Apple', inferred_city='New York City'),\n    City(text='SF', inferred_city='San Francisco'),\n    City(text='San Fran', inferred_city='San Francisco'),\n    City(text='San Francisco', inferred_city='San Francisco')\n]\n</code></pre> <p>In this example, we'll explore how you can do text and entity deduplication from a piece of text. </p>"},{"location":"examples/deduplication/#creating-our-data-model","title":"Creating our data model","text":"<p>To extract and deduplicate entities, we'll want to think carefully about the data we want to extract from this text. We clearly want a <code>list</code> of <code>cities</code>. So we'll want to create a data model to represent a city. But we won't stop there:  we don't want to just get a list of cities that appear in the text. We want to get an mapping or understanding that SF is the same as San Francisco, and the Big Apple is the same as New York City, etc. </p> <pre><code>import pydantic\n\nclass City(pydantic.BaseModel):\n    '''\n        A model to represent a city.\n    '''\n\n    text: str = pydantic.Field(\n        description = 'The city name as it appears'\n    )\n\n    inferred_city: str = pydantic.Field(\n            description = 'The inferred and normalized city name.'\n        )\n</code></pre>"},{"location":"examples/deduplication/#creating-our-prompt","title":"Creating our prompt","text":"<p>Now we'll need to use this model and convert it into a prompt we can send to a language model. We'll use Marvin's prompt_fn to let us write a prompt like a python function. </p> <pre><code>from marvin import prompt_fn\n\n@prompt_fn\ndef get_cities(text: str) -&gt; list[City]:\n    '''\n        Expertly deduce and infer all cities from the follwing text: {{ text }}\n    '''\n</code></pre> What does get_cities do under the hood? <p>Marvin's <code>prompt_fn</code> only creates a prompt to send to a large language model. It does not call any  external service, it's simply responsible for translating your query into something that a  large language model will understand. </p> <p>Here's the output when we plug in our sentence from above:</p> <pre><code>get_cities(\"Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco.\")\n</code></pre> Click to see output <pre><code>{\n    \"tools\": [\n        {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"FormatResponse\",\n            \"parameters\": {\n            \"$defs\": {\n                \"City\": {\n                \"description\": \"A model to represent a city.\",\n                \"properties\": {\n                    \"text\": {\n                    \"description\": \"The city name as it appears\",\n                    \"title\": \"Text\",\n                    \"type\": \"string\"\n                    },\n                    \"inferred_city\": {\n                    \"description\": \"The inferred and normalized city name.\",\n                    \"title\": \"Inferred City\",\n                    \"type\": \"string\"\n                    }\n                },\n                \"required\": [\n                    \"text\",\n                    \"inferred_city\"\n                ],\n                \"title\": \"City\",\n                \"type\": \"object\"\n                }\n            },\n            \"properties\": {\n                \"data\": {\n                \"description\": \"The data to format.\",\n                \"items\": {\n                    \"$ref\": \"#/$defs/City\"\n                },\n                \"title\": \"Data\",\n                \"type\": \"array\"\n                }\n            },\n            \"required\": [\n                \"data\"\n            ],\n            \"type\": \"object\"\n            }\n        }\n        }\n    ],\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\n        \"name\": \"FormatResponse\"\n        }\n    },\n    \"messages\": [\n        {\n        \"content\": \"\n            Expertly deduce and infer all cities from the follwing text: \n            Chicago, The Windy City, New York City, the Big Apple, SF, \n            San Fran, San Francisco.\n            \",\n        \"role\": \"system\"\n        }\n    ]\n    }   \n</code></pre>"},{"location":"examples/deduplication/#calling-our-language-model","title":"Calling our Language Model","text":"<p>Let's see what happens when we actually call our Large Language Model. Below, <code>**</code> tells let's us pass the prompt's parameters into our call to OpenAI.</p> <pre><code>from openai import OpenAI\nimport json\n\nclient = OpenAI(api_key = 'YOUR_OPENAI_KEY')\n\nresponse = client.chat.completions.create(\n    model = 'gpt-3.5-turbo',\n    temperature = 0,\n    **get_cities(\n        (\n            \"Chicago, The Windy City, New York City, \"\n            \"The Big Apple, SF, San Fran, San Francisco.\"\n        )\n    )\n)\n</code></pre> View the raw response <p>The raw response we receive looks like  <pre><code>{\n\"id\": \"chatcmpl-id-number\",\n\"choices\": [\n    {\n    \"finish_reason\": \"stop\",\n    \"index\": 0,\n    \"message\": {\n        \"content\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": [\n        {\n            \"id\": \"call-id-number\",\n            \"function\": {\n            \"arguments\": \"{\\n  \\\"data\\\": [\\n    {\\n      \\\"text\\\": \\\"Chicago\\\",\\n      \\\"inferred_city\\\": \\\"Chicago\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"The Windy City\\\",\\n      \\\"inferred_city\\\": \\\"Chicago\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"New York City\\\",\\n      \\\"inferred_city\\\": \\\"New York City\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"The Big Apple\\\",\\n      \\\"inferred_city\\\": \\\"New York City\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"SF\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"San Fran\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"San Francisco\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    }\\n  ]\\n}\",\n            \"name\": \"FormatResponse\"\n            },\n            \"type\": \"function\"\n        }\n        ]\n    }\n    }\n],\n\"created\": 1702591915,\n\"model\": \"gpt-3.5-turbo-0613\",\n\"object\": \"chat.completion\",\n\"system_fingerprint\": null,\n\"usage\": {\n    \"completion_tokens\": 165,\n    \"prompt_tokens\": 95,\n    \"total_tokens\": 260\n}\n}   \n</code></pre></p> <p>We can parse the raw response and mine out the relevant responses, </p> <pre><code>[\n    City(**city)\n    for city in \n    json.loads(\n        response.choices[0].message.tool_calls[0].function.arguments\n    ).get('output')\n]\n</code></pre> <p>what we'll get now is the pairs of raw, observed city and cleaned deduplicated city.</p> <pre><code>[\n    City(text='Chicago', inferred_city='Chicago'),\n    City(text='The Windy City', inferred_city='Chicago'),\n    City(text='New York City', inferred_city='New York City'),\n    City(text='The Big Apple', inferred_city='New York City'),\n    City(text='SF', inferred_city='San Francisco'),\n    City(text='San Fran', inferred_city='San Francisco'),\n    City(text='San Francisco', inferred_city='San Francisco')\n]\n</code></pre> <p>So, we've seen that deduplicating data with a Large Language Model is fairly straightforward in a customizable way using Marvin. If you want the entire content of the cells above in  one place, you can copy the cell below.</p>"},{"location":"examples/extraction_api/","title":"Basic Extraction API","text":"<p>With Marvin, you can easily build a production-grade application or data pipeline to extract structured data from unstructured text.  In this example, we'll show how to </p> <ul> <li>Write an AI-powered Function.</li> <li>Build a production-ready API.</li> </ul>"},{"location":"examples/extraction_api/#writing-an-ai-powered-function","title":"Writing an AI-powered Function.","text":"<p>Example</p> <p>Marvin translates your Python code into English, passes that to an Large Language Model,  and parses its response. It uses AI to evaluate your function, no code required.</p> <p>Let's build a function that extracts a person's first_name, last_name, and age  from a paragraph of text. This is typically a pretty daunting task for a machine learning savant, much less your average engineer. Marvin lets you accomplish this by write code  the way you normally would, no PhD required.</p> <p>We'll simply write a Python function, tell it that we expect a <code>text</code> input and that it'll  output a <code>dict</code>, or dictionary. With Marvin, we'll use <code>ai_fn</code> and decorate his function.  When we do, this function will use AI to get its answer.</p> <p><pre><code>from marvin import ai_fn, settings\nfrom typing import Any\n\nsettings.openai.api_key = 'API_KEY' \n\n@ai_fn\ndef extract_person(text: str) -&gt; dict:\n    '''\n        Correctly infers a persons `birth_year`, `first_name` and `last_name`\n        from the passed `text`. \n    '''\n</code></pre> This function can now be run! When we test it out, we get great results.</p> Results <pre><code>extract_person('My name is Peter Parker, and I was born when Clinton was first elected')\n\n# returns {'first_name': 'Peter', 'last_name': 'Parker', 'birth_year': 1992}\n</code></pre>"},{"location":"examples/extraction_api/#build-a-production-ready-api","title":"Build a production-ready API.","text":"<p>In the following example, we will demonstrate how to deploy the AI function we just created as an API using FastAPI. FastAPI is a powerful tool that allows us to easily turn our AI function into a fully-fledged API. This API can then be used by anyone to send a POST request to our <code>/extract_person/</code> endpoint and get the structured data they need. Let's see how this can be done.</p> <p>Example</p> <p>Now that we have our AI function, let's deploy it as an API using FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, settings\n\napp = FastAPI()\n\nsettings.openai.api_key = 'API_KEY'\n\n@app.post(\"/extract_person/\")\n@ai_fn\ndef extract_person(text: str) -&gt; dict:\n    '''\n        Correctly infers a persons `birth_year`, `first_name` and `last_name`\n        from the passed `text`. \n    '''\n</code></pre> <p>With just a few lines of code, we've turned our AI function into a fully-fledged API. Now, anyone can send a POST request to our <code>/extract_person/</code> endpoint and get the structured data they need.</p> API Deployment <p><pre><code>import uvicorn\nimport asyncio\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nasyncio.run(server.serve())\n</code></pre> Now, you can navigate to localhost:8000/docs to interact with your API.</p> Making Requests <p><pre><code>import requests\n\ndata = {\"text\": \"My name is Peter Parker, and I was born when Clinton was first elected\"}\nresponse = requests.post(\"http://localhost:8000/extract_person/\", json=data)\nprint(response.json())\n\n# returns {'first_name': 'Peter', 'last_name': 'Parker', 'birth_year': 1992}\n</code></pre> This will send a POST request to the <code>/extract_person/</code> endpoint with the provided text and print the response.</p>"},{"location":"examples/github_digest/","title":"GitHub Digest","text":"<p>A fun example covering a few practical patterns is to create an AI digest of GitHub activity for a given repo.</p> <p>If you've spent some time messing with AI tools in the Python ecosystem lately, you're probably familiar with Jinja2. Jinja pairs really nicely with LLMs, because you can structure the template in a way that either makes it easy for the LLM to fill in the blanks, or makes it easy for you to fill in the blanks with traditional software and then pass the rendered template to the LLM as a prompt.</p> <p>Here's an example of the latter:</p>"},{"location":"examples/github_digest/#writing-an-epic-about-the-days-events-in-prefecthqprefect","title":"Writing an epic about the day's events in <code>PrefectHQ/prefect</code>","text":"<p>The AI part is pretty much English:</p> <pre><code>@ai_fn(\n    instructions=\"You are a witty and subtle orator. Speak to us of the day's events.\"\n)\ndef summarize_digest(markdown_digest: str) -&gt; str:\n    \"\"\"Given a markdown digest of GitHub activity, create a Story that is\n    informative, entertaining, and epic in proportion to the day's events -\n    an empty day should be handled with a short sarcastic quip about humans\n    and their laziness.\n\n    The story should capture collective efforts of the project.\n    Each contributor plays a role in this story, their actions\n    (issues raised, PRs opened, commits merged) shaping the events of the day.\n\n    The narrative should highlight key contributors and their deeds, drawing upon the\n    details in the digest to create a compelling and engaging tale of the day's events.\n    A dry pun or 2 are encouraged.\n\n    Usernames should be markdown links to the contributor's GitHub profile.\n\n    The story should begin with a short pithy welcome to the reader and have\n    a very short, summarizing title.\n    \"\"\" # noqa: E501 (to make Ruff happy)\n</code></pre> <p>... and the rest is just... Python?</p> <pre><code>import os\nimport inspect\nfrom datetime import date, datetime, timedelta\n\nfrom marvin import ai_fn\n\nfrom my_helpers import (\n    fetch_contributor_data,\n    post_slack_message,\n    YOUR_JINJA_TEMPLATE,\n)\n\nasync def daily_github_digest(\n    owner: str = \"PrefectHQ\",\n    repo: str = \"marvin\",\n    slack_channel: str = \"ai-tools\",\n    gh_token_env_var: str = \"GITHUB_PAT\",\n):\n    since = datetime.utcnow() - timedelta(days=1)\n\n    data = await fetch_contributor_data(\n        token=os.getenv(gh_token_env_var), # load from your secrets manager\n        owner=owner,\n        repo=repo,\n        since=since,\n    )\n\n    markdown_digest = YOUR_JINJA_TEMPLATE.render(\n        today=date.today(),\n        owner=owner,\n        repo=repo,\n        contributors_activity=data,\n    )\n\n    epic_story = summarize_digest(markdown_digest)\n\n    await post_slack_message(\n        message=epic_story,\n        channel=slack_channel,\n    )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(daily_github_digest(owner=\"PrefectHQ\", repo=\"prefect\"))\n</code></pre> <p>Tip</p> <p>I brought some helpers with me to make this easier:</p> <ul> <li><code>fetch_contributor_data</code> uses the GitHub API to get a list of contributors and their activity</li> <li><code>post_slack_message</code> uses the Slack API to post a message to a channel</li> <li><code>YOUR_JINJA_TEMPLATE</code> is a Jinja template that you can use to render the digest</li> </ul> <p>Find my helpers here</p> <p>Here's a sample output from August 16th 2023 on the <code>PrefectHQ/prefect</code> repo: <pre><code>Greetings, wanderer! Sit, rest your feet, and allow me to regale you with the\nheroic deeds of the PrefectHQ/prefect repository on this fateful day, the 16th\nof August, 2023.\n\nOur tale begins with the indefatigable jakekaplan, who single-handedly opened \ntwo perceptive Pull Requests: flow run viz v2 and don't run tasks draft-. Not\nsatisfied, he also valiantly merged five commits, clearing the path for others\nto tread. Meanwhile, cicdw, desertaxle, and serinamarie, each merged a single,\nbut significant commit, contributing their mite to the collective effort.\n\nIn the realm of PRs, prefectcboyd, WillRaphaelson, and bunchesofdonald all \nunfurled their banners, each opening a PR of their own, like pioneers staking\ntheir claim on the wild frontiers of code.\n\nIn the grand theatre of software development, where victory is measured in merged\ncommits and opened PRs, these individuals stood tall, their actions resonating \nthrough the corridors of GitHub. Their names are etched onto the scroll of this \nday, their deeds a testament to their commitment.\n\nSo ends the telling of this day's events. Until our paths cross again, wanderer, \nmay your code compile and your tests always pass!\n</code></pre></p>"},{"location":"examples/github_digest/#full-example","title":"Full example:","text":"<p>The full example, with improvements like caching and observability, can be found here.</p>"},{"location":"examples/slackbot/","title":"Build a Slack bot with Marvin","text":""},{"location":"examples/slackbot/#slack-setup","title":"Slack setup","text":"<p>Get a Slack app token from Slack API and add it to your <code>~/.marvin/.env</code> file:</p> <pre><code>MARVIN_SLACK_API_TOKEN=your-slack-bot-token\n</code></pre> <p>Choosing scopes</p> <p>You can choose the scopes you need for your bot in the OAuth &amp; Permissions section of your Slack app.</p>"},{"location":"examples/slackbot/#building-the-bot","title":"Building the bot","text":""},{"location":"examples/slackbot/#define-a-fastapi-app-to-handle-slack-events","title":"Define a FastAPI app to handle Slack events","text":"<p><pre><code>@app.post(\"/chat\")\nasync def chat_endpoint(request: Request):\n    payload = SlackPayload(**await request.json())\n    match payload.type:\n        case \"event_callback\":\n            asyncio.create_task(handle_message(payload))\n        case \"url_verification\":\n            return {\"challenge\": payload.challenge}\n        case _:\n            raise HTTPException(400, \"Invalid event type\")\n\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=4200)\n</code></pre> Here, we define a simple FastAPI endpoint / app to handle Slack events and return a response. We run our interesting logic in the background using <code>asyncio.create_task</code> to make sure we return <code>{\"status\": \"ok\"}</code> within 3 seconds, as required by Slack.</p>"},{"location":"examples/slackbot/#handle-generating-the-ai-response","title":"Handle generating the AI response","text":"<p>I like to start with this basic structure, knowing that one way or another...</p> <pre><code>async def handle_message(payload: dict) -&gt; str:\n    # somehow generate the ai responses\n    ...\n\n    # post the response to slack\n    _post_message(\n        messsage=some_message_ive_constructed,\n        channel=event.get(\"channel\", \"\"),\n        thread_ts=thread_ts,\n    )\n</code></pre> <p>... I need to take in a Slack app mention payload, generate a response, and post it back to Slack.</p>"},{"location":"examples/slackbot/#a-couple-considerations","title":"A couple considerations","text":"<ul> <li>do I want the bot to respond to users in a thread or in the channel?</li> <li>do I want the bot to have memory of previous messages? how so?</li> <li>what tools do I need to generate accurate responses for my users?</li> </ul> <p>In our case of the Prefect Community slackbot, we want:</p> <ul> <li>the bot to respond in a thread</li> <li>the bot to have memory of previous messages by slack thread</li> <li>the bot to have access to the internet, GitHub, embedded docs, etc</li> </ul>"},{"location":"examples/slackbot/#example-implementation-of-handler-prefect-community-slackbot","title":"Example implementation of handler: Prefect Community Slackbot","text":"<p>This runs 24/7 in the #ask-marvin channel of the Prefect Community Slack. It responds to users in a thread, and has memory of previous messages by slack thread. It uses the <code>chroma</code> and <code>github</code> tools for RAG to answer questions about Prefect 2.x.</p> <pre><code>async def handle_message(payload: SlackPayload):\n    logger = get_logger(\"slackbot\")\n    user_message = (event := payload.event).text\n    cleaned_message = re.sub(BOT_MENTION, \"\", user_message).strip()\n    logger.debug_kv(\"Handling slack message\", user_message, \"green\")\n    if (user := re.search(BOT_MENTION, user_message)) and user.group(\n        1\n    ) == payload.authorizations[0].user_id:\n        thread = event.thread_ts or event.ts\n        assistant_thread = CACHE.get(thread, Thread())\n        CACHE[thread] = assistant_thread\n\n        await handle_keywords.submit(\n            message=cleaned_message,\n            channel_name=await get_channel_name(event.channel),\n            asking_user=event.user,\n            link=(  # to user's message\n                f\"{(await get_workspace_info()).get('url')}archives/\"\n                f\"{event.channel}/p{event.ts.replace('.', '')}\"\n            ),\n        )\n\n        with Assistant(\n            name=\"Marvin (from Hitchhiker's Guide to the Galaxy)\",\n            tools=[task(multi_query_chroma), task(search_github_issues)],\n            instructions=(\n                \"use chroma to search docs and github to search\"\n                \" issues and answer questions about prefect 2.x.\"\n                \" you must use your tools in all cases except where\"\n                \" the user simply wants to converse with you.\"\n            ),\n        ) as assistant:\n            user_thread_message = await assistant_thread.add_async(cleaned_message)\n            await assistant_thread.run_async(assistant)\n            ai_messages = assistant_thread.get_messages(\n                after_message=user_thread_message.id\n            )\n            await task(post_slack_message)(\n                ai_response_text := \"\\n\\n\".join(\n                    m.content[0].text.value for m in ai_messages\n                ),\n                channel := event.channel,\n                thread,\n            )\n            logger.debug_kv(\n                success_msg := f\"Responded in {channel}/{thread}\",\n                ai_response_text,\n                \"green\",\n            )\n</code></pre> <p>This is just an example</p> <p>There are many ways to implement a Slackbot with Marvin's Assistant SDK / utils, FastAPI is just our favorite.</p> <p>Run this file with something like: <pre><code>python start.py\n</code></pre></p> <p>... and navigate to <code>http://localhost:4200/docs</code> to see your bot's docs.</p> <p>This is now an endpoint that can be used as a Slack event handler. You can use a tool like ngrok to expose your local server to the internet and use it as a Slack event handler.</p>"},{"location":"examples/slackbot/#building-an-image","title":"Building an image","text":"<p>Based on this example, one could write a <code>Dockerfile</code> to build a deployable image:</p> <p><pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY . /app\n\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y git build-essential &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN pip install \".[slackbot]\"\n\nEXPOSE 4200\n\nCMD [\"python\", \"cookbook/slackbot/start.py\"]\n</code></pre> Note that we're installing the <code>slackbot</code> extras here, which are required for tools used by the worker bot defined in this example's <code>cookbook/slackbot/start.py</code> file.</p>"},{"location":"examples/slackbot/#find-the-whole-example-here","title":"Find the whole example here","text":"<ul> <li>cookbook/slackbot/start.py</li> </ul>"},{"location":"help/legacy_docs/","title":"Legacy Documentation","text":"<p>If you want to view documentation of previous versions of Marvin, here's a step by step guide on how to do that.</p>"},{"location":"help/legacy_docs/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>Git</code></li> <li><code>python3</code></li> <li><code>pip</code></li> </ul>"},{"location":"help/legacy_docs/#automated-script-for-legacy-documentation","title":"Automated Script for Legacy Documentation","text":"<p>To build and view the docs for a specific version of Marvin, you can use this script.</p> <p>You can either clone the Marvin repo and run the script locally, or copy the script and run it directly in your terminal after making it executable: <pre><code># unix\nchmod +x scripts/serve_legacy_docs\n\n# run the script (default version is v1.5.6)\n./scripts/serve_legacy_docs\n\n# optionally, specify a version\n./scripts/serve_legacy_docs v1.5.3\n</code></pre></p>"},{"location":"help/legacy_docs/#manual-steps","title":"Manual Steps","text":"<p>If you prefer to manually perform the steps or need to tailor them for your specific operating system, follow these instructions:</p> <ol> <li> <p>Clone the Repository    Clone the Marvin repository using Git:    <pre><code>git clone https://github.com/PrefectHQ/marvin.git\ncd marvin\n</code></pre></p> </li> <li> <p>Checkout the Specific Tag    Checkout the tag for the version you are interested in. Replace <code>v1.5.6</code> with the desired version tag:    <pre><code>git fetch --tags\ngit checkout tags/v1.5.6\n</code></pre></p> </li> <li> <p>Create a Virtual Environment    Create and activate a virtual environment to isolate the dependency installation:    <pre><code>python3 -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n</code></pre></p> </li> <li> <p>Install Dependencies    Install the necessary dependencies for the documentation:    <pre><code>pip install -e \".[dev,docs]\"\n</code></pre></p> </li> <li> <p>Serve the Documentation Locally    Use <code>mkdocs</code> to serve the documentation:    <pre><code>mkdocs serve\n</code></pre>    This will start a local server. View the documentation by navigating to <code>http://localhost:8000</code> in your web browser.</p> </li> <li> <p>Exit Virtual Environment    Once finished, you can exit the virtual environment:    <pre><code>deactivate\n</code></pre></p> </li> </ol> <p>Optionally, you can remove the virtual environment folder:    <pre><code>rm -rf venv\n</code></pre></p>"},{"location":"prompting/prompt_function/","title":"Prompt function","text":"<p>Marvin puts the engineering in prompt engineering. We expose a low-level <code>prompt_fn</code> decorator that lets you write prompts as functions. This lets you build fully type-hinted prompts that other engineers can introspect, version, and test.</p> <p>This is the easiest way to use Azure / OpenAI's function calling API.</p>"},{"location":"prompting/prompt_function/#basic-use","title":"Basic Use","text":""},{"location":"prompting/prompt_function/#type-hinting","title":"Type Hinting","text":"<p>Example</p> <p>Marvin translates your Python code into English. We'll simply write a Python function,  tell it that we expect an integer input <code>n</code> input and that it'll  output <code>list[str]</code>, or a list of strings. With Marvin, we'll use <code>prompt_fn</code> and decorate this function.  When we do, this function can be cast to a payload that can be send to an LLM.</p> <p><pre><code>from marvin.prompts import prompt_fn\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"type\": \"string\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ]\n        },\n        \"name\": \"Output\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Output\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#advanced-use","title":"Advanced Use","text":""},{"location":"prompting/prompt_function/#use-with-pydantic","title":"Use with Pydantic","text":"<p>Example</p> <p>Marvin supports type-hinting with Pydantic, so your return annotation can be a more complex data-model.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/Fruit\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ],\n            \"definitions\": {\n            \"Fruit\": {\n                \"title\": \"Fruit\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"FruitList\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"FruitList\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#full-customization","title":"Full Customization","text":"<p>Example</p> <p>Marvin supports full customization of every element of your prompts. You can customize the  name, description, and field_names of your <code>response_model</code>. </p> <p>Say we want to change the task to generate in Swedish.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn(\n    response_model_name = 'Fruktlista', \n    response_model_description = 'A list of fruits in Swedish',\n    response_model_field_name = 'Frukt'\n)\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"Frukt\": {\n                \"title\": \"Frukt\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/Fruit\"\n                }\n            }\n            },\n            \"required\": [\n            \"Frukt\"\n            ],\n            \"definitions\": {\n            \"Fruit\": {\n                \"title\": \"Fruit\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"Fruktlista\",\n        \"description\": \"A list of fruits in Swedish\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Fruktlista\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#referencing-globals","title":"Referencing Globals","text":"<p>Example</p> <p>Marvin passes the name of your response model to the prompt for you to reference as a convenience.</p> <p>Say we want to change the task to generate in Swedish.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn(response_model_name = 'Fruits')\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} {{ response_model.__name__.lower() }}'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"Generates a list of 3 blue fruits\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"$ref\": \"#/definitions/Fruit\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ],\n                \"definitions\": {\n                \"Fruit\": {\n                    \"title\": \"Fruit\",\n                    \"type\": \"object\",\n                    \"properties\": {\n                    \"color\": {\n                        \"title\": \"Color\",\n                        \"type\": \"string\"\n                    }\n                    },\n                    \"required\": [\n                    \"color\"\n                    ]\n                }\n                }\n            },\n            \"name\": \"Fruits\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Fruits\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#contexts","title":"Contexts","text":"<p>Example</p> <p>Marvin supports full passing context dictionaries to your prompt's rendering environment.</p> <p>Say we want to list 'seasonal' fruits. We'll pass the datetime.</p> <pre><code>from marvin.prompts import prompt_fn\nfrom datetime import date\n\n@prompt_fn(ctx = {'today': date.today()})\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    ''' \n    Generates a list of {{ n }} {{ color }} fruits in season.\n        - The date is {{ today }}\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"Generates a list of 3 blue fruits in season.\\n- The date is 2023-09-22\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"type\": \"string\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ]\n            },\n            \"name\": \"Output\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Output\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#multi-turn-prompts","title":"Multi-Turn Prompts","text":"<p>Example</p> <p>Marvin supports multi-turn conversations. If no role is specified, the whole block is assumed to be a system prompt. To override this default behavior, simply break into Human, System, Assistant turns. </p> <pre><code>from marvin.prompts import prompt_fn\nfrom datetime import date\n\n@prompt_fn(ctx = {'today': date.today()})\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    ''' \n    System: You generate a list of {{ n }} fruits in season.\n        - The date is {{ today }}\n\n    User: I want {{ color }} fruits only.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"You generate a list of 3 fruits in season.\\n- The date is 2023-09-22\"\n            },\n            {\n            \"role\": \"user\",\n            \"content\": \"I want blue fruits.\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"type\": \"string\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ]\n            },\n            \"name\": \"Output\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Output\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#use-cases","title":"Use Cases","text":""},{"location":"prompting/prompt_function/#classification","title":"Classification","text":"<p>Example</p> <p>Marvin supports multi-turn conversations. If no role is specified, the whole block is assumed to be a system prompt. To override this default behavior, simply break into Human, System, Assistant turns. </p> <pre><code>from marvin.prompts import prompt_fn\nfrom typing import Optional\nfrom enum import Enum\n\nclass Food(Enum):\n    '''\n        Food classes\n    '''\n    FRUIT = 'Fruit'\n    VEGETABLE = 'Vegetable'\n\n@prompt_fn\ndef classify_fruits(food: str) -&gt; Food:\n    ''' \n        Expertly determines the class label of {{ food }}.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>classify_fruits('tomato')</code> <pre><code>{\n\"messages\": [\n    {\n    \"role\": \"system\",\n    \"content\": \"Expertly determines the class label of tomato.\"\n    }\n],\n\"functions\": [\n    {\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n        \"output\": {\n            \"$ref\": \"#/definitions/Food\"\n        }\n        },\n        \"required\": [\n        \"output\"\n        ],\n        \"definitions\": {\n        \"Food\": {\n            \"title\": \"Food\",\n            \"description\": \"Food classes\",\n            \"enum\": [\n            \"Fruit\",\n            \"Vegetable\"\n            ]\n        }\n        }\n    },\n    \"name\": \"Output\",\n    \"description\": \"\"\n    }\n],\n\"function_call\": {\n    \"name\": \"Output\"\n}\n}   \n</code></pre>"},{"location":"prompting/prompt_function/#entity-extraction","title":"Entity Extraction","text":"<p>Example</p> <p>In this example, Marvin is configured to perform entity extraction on a list of fruits mentioned in a text string. The function <code>extract_fruits</code> identifies and extracts fruit entities based on the input text. These entities are then returned as a list of Pydantic models.</p> <pre><code>from marvin.prompts import prompt_fn\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass FruitEntity(BaseModel):\n    '''\n        Extracted Fruit Entities\n    '''\n    name: str\n    color: str\n\n@prompt_fn\ndef extract_fruits(text: str) -&gt; List[FruitEntity]:\n    ''' \n        Extracts fruit entities from the given text: {{ text }}.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> <code>extract_fruits('There are red apples and yellow bananas.')</code> <pre><code>{\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Extracts fruit entities from the given text: There are red apples and yellow bananas..\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/FruitEntity\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ],\n            \"definitions\": {\n            \"FruitEntity\": {\n                \"title\": \"FruitEntity\",\n                \"description\": \"Extracted Fruit Entities\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"name\": {\n                    \"title\": \"Name\",\n                    \"type\": \"string\"\n                },\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"name\",\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"Output\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Output\"\n    }\n    }\n</code></pre>"},{"location":"utilities/chat_completion/","title":"Chat completion","text":"<p>In Marvin, each supported Large Language Model can be accessed with one common API. This means that  you can easily switch between providers without having to change your code. We have anchored our API  to mirror that of OpenAI's Python SDK. </p> <p>In plain English.</p> <ul> <li>A drop-in replacement for OpenAI's ChatCompletion, with sensible superpowers.</li> <li>You can use Anthropic and other Large Language Models as if you were using OpenAI.</li> </ul>"},{"location":"utilities/chat_completion/#basic-use","title":"Basic Use","text":"<p>Using a single interface to multiple models helps reduce boilerplate code and translation. In  the current era of building with different LLM providers, developers often need to rewrite their code just to use a new model. With Marvin you can simply import ChatCompletion and  specify a model name.</p> <p>Example: Specifying a Model</p> <p>We first past the API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import ChatCompletion\nimport os\n\nos.environ['OPENAI_API_KEY'] = 'openai_private_key'\nos.environ['ANTHROPIC_API_KEY'] = 'anthropic_private_key'\n</code></pre> ChatCompletion recognizes the model name and correctly routes it to the correct provider. So  you can simply pass 'gpt-3.5-turbo' or 'claude-2' and it just works.</p> <p><pre><code># Set up a dummy list of messages.\nmessages = [{'role': 'user', 'content': 'Hey! How are you?'}]\n\n# Call gpt-3.5-turbo simply by specifying it inside of ChatCompletion.\nopenai = ChatCompletion('gpt-3.5-turbo').create(messages = messages)\n\n# Call claude-2 simply by specifying it inside of ChatCompletion.\nanthropic = ChatCompletion('claude-2').create(messages = messages)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>print(openai.choices[0].message.content)\n# Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?\n\nprint(anthropic.choices[0].message.content)\n# I'm doing well, thanks for asking!\n</code></pre> <p>You can set more than just the model and provider as a default value. Any keyword arguments passed to ChatCompletion will be persisted and passed to subsequent requests.</p> <p>Example: Frozen Model Facets</p> <p><pre><code># Create system messages or conversation history to seed.\nsystem_messages = [{'role': 'system', 'content': 'You talk like a pirate'}]\n\n# Instatiate gpt-3.5.turbo with the previous system_message. \nopenai_pirate = ChatCompletion('gpt-3.5.turbo', messages = system_messages)\n\n# Call the instance with create. \nopenai_pirate.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre> For functions and messages, this will concatenate the frozen and passed arguments. All other passed keyword arguments will overwrite the default settings.</p> <pre><code>print(openai_pirate.choices[0].message.content)\n# Arrr, matey! I be doin' well on this fine day. How be ye farein'?\n</code></pre> <p>Replacing OpenAI's ChatCompletion.</p> <p>ChatCompletion is designed to be a drop-in replacement for OpenAI's ChatCompletion.  Just import openai from marvin or, equivalently, ChatCompletion from marvin.openai. </p> <pre><code>from marvin import openai\n\n\nopenai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre>"},{"location":"utilities/chat_completion/#advanced-use","title":"Advanced Use","text":""},{"location":"utilities/chat_completion/#response-model","title":"Response Model","text":"<p>With Marvin, you can get structured outputs from model providers by passing a response type. This lets developers write prompts with Python objects, which are easier to develop, version, and test than language.</p> <p>In plain English.</p> <p>You can specify a type, struct, or data model to ChatCompletion, and Marvin will ensure the model's response adheres to that type.</p> <p>Let's consider two examples.</p> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass CoffeeOrder(BaseModel):\n    size: Literal['small', 'medium', 'large']\n    milk: Literal['soy', 'oat', 'dairy']\n    with_sugar: bool = False\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = CoffeeOrder\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# CoffeeOrder(size='small', milk='soy', with_sugar=False)\n</code></pre> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass Translation(BaseModel):\n    spanish: str\n    french: str\n    swedish: str\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [\n    {\n        'role': 'system',\n        'content': 'You translate user messages into other languages.'\n    },\n    {\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = Translation\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# Translation(\n#   spanish='\u00bfPuedo conseguir un caf\u00e9 con leche de soja peque\u00f1o?', \n#   french='Puis-je avoir un petit latte au lait de soja ?', \n#   swedish='Kan jag f\u00e5 en liten sojamj\u00f6lklatt\u00e9?'\n# )\n</code></pre>"},{"location":"utilities/chat_completion/#function-calling","title":"Function Calling","text":"<p>ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate. </p> <p>Marvin lets you pass your choice of JSON Schema or Python functions directly to ChatCompletion. It does the right thing.</p> <p>In plain English.</p> <p>You can pass regular Python functions to ChatCompletion, and Marvin will take care of serialization of that function using <code>Pydantic</code> in a way you can customize.</p> <p>Let's consider an example.</p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We have the usual annuity formula from accounting, which we can write deterministically. We wouldn't expect an LLM to be able to both handle semantic parsing and math in one fell swoop, so we want to pass it a hardcoded function so it's only task is to compute its arguments. <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put it $100 every month for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 4495.5}\n</code></pre> <p>In the case where several functions are passed. It does the right thing. </p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We want to give it another tool from  accounting 101: the ability to compute compound interest. It'll now have to tools to choose from: <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n\ndef compound_interest(P: float, r: float, t: float, n: int) -&gt; float:\n    \"\"\"\n    This function calculates and returns the total amount of money \n    accumulated after n times compounding interest per year at an annual \n    interest rate of r for a period of t years on an initial amount of P.\n    \"\"\"\n    A = P * (1 + r/n)**(n*t)\n    return round(A,2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'If I have $5000 in my account today and leave it in for 5 years at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'compound_interest', 'content': 8811.71}\n</code></pre> <p>Of course, we if ask if about repeated deposits, it'll correctly call the right function.</p> <pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put in $50/mo for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n\nresponse.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 2247.75}\n</code></pre>"},{"location":"utilities/chat_completion/#chaining","title":"Chaining","text":"<p>Above we saw how ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate.</p> <p>Often we want to take the output of a function call and pass it back to an LLM so that it can either call a new function or summarize the results of what we've computed for it. This agentic pattern is easily enabled with Marvin. </p> <p>Rather than write while- and for- loops for you, we've made ChatCompletion a context manager. This lets you maintain a state of a conversation that you can send and receive messages from. You have complete control over the internal logic.</p> <p>In plain English.</p> <p>You can have a conversation with an LLM, exposing functions for it to use in service of your request.  Marvin maintains state to make it easier to maintain and observe this conversation.</p> <p>Let's consider an example.</p> <p>Example: Chaining</p> <p>Let's build a simple arithmetic bot. We'll empower with arithmetic operations, like <code>add</code> and <code>divide</code>. We'll seed it with an arithmetic question.</p> <p><pre><code>from marvin import openai\nopenai.api_key = 'secret_key'\n\ndef divide(x: float, y: float) -&gt; str:\n    '''Divides x and y'''\n    return str(x/y)\n\ndef add(x: int, y: int) -&gt; str:\n    '''Adds x and y'''\n    return str(x+y)\n\nwith openai.ChatCompletion(functions = [add, divide]) as conversation:\n\n    # Start off with an external question / prompt. \n    prompt = 'What is 4124124 + 424242 divided by 48124?'\n\n    # Initialize the conversation with a prompt from the user. \n    conversation.send(messages = [{'role': 'user', 'content': prompt}])\n\n    # While the most recent turn has a function call, evaluate it. \n    while conversation.last_response.has_function_call():\n\n        # Send the most recent function call to the conversation. \n        conversation.send(messages = [\n            conversation.last_response.call_function() \n        ])\n</code></pre> The context manager, which we've called conversation (you can call it whatever you want), holds every turn of the conversation which we can inspect. </p> <pre><code>conversation.last_response.choices[0].message.content\n\n# The result of adding 4124124 and 424242 is 4548366. When this result is divided by 48124, \n# the answer is approximately 94.51346521486161.\n</code></pre> <p>If we want to see the entire state, every <code>[request, response]</code> pair is held in the conversation's  <code>turns</code>. <pre><code>[response.choices[0].message for response in conversation.turns]\n\n# [&lt;OpenAIObject at 0x120667c50&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"add\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4124124,\\n  \\\"y\\\": 424242\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4830&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"divide\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4548366,\\n  \\\"y\\\": 48124\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4b90&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": \"The result of adding 4124124 and 424242 is 4548366. \n#             When this result is divided by 48124, the answer is \n#             approximately 94.51346521486161.\"\n# }]\n</code></pre></p>"},{"location":"welcome/installation/","title":"Installation","text":"<p>You can install Marvin with <code>pip</code> (note that Marvin requires Python 3.9+):</p> <pre><code>pip install marvin\n</code></pre> <p>To verify your installation, run <code>marvin --help</code> in your terminal. </p> <p>You can upgrade to the latest released version at any time:</p> <pre><code>pip install marvin -U\n</code></pre>"},{"location":"welcome/installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing docs for instructions on installing Marvin for development.</p>"},{"location":"welcome/overview/","title":"Marvin Documentation","text":"<p>Marvin is a Python library that lets you use Large Language Models by writing code, not prompts. It's open source, free to use, rigorously type-hinted, used by thousands of engineers, and built by the engineering team at Prefect.</p> <p>Marvin is lightweight and is built for incremental adoption. You can use it purely as a serialization library and bring your own stack, or fully use its engine to work with OpenAI and other providers. </p> What Marvin feels like. Structured Data ExtractionText ClassificationBusiness Logic <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin.components import ai_model\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n    latitude: float\n    longitude: float\n\nai_model(Location)(\"They say they're from the Windy City!\")\n# Location(city='Chicago', state='Illinois', latitude=41.8781, longitude=-87.6298)\n</code></pre> Notice there's no code written, just the expected types. Marvin's components turn your function into a prompt, uses AI to get its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin import ai_classifier\nfrom typing import Literal\n\n@ai_classifier\ndef customer_intent(text: str) -&gt; Literal['Store Hours', 'Pharmacy', 'Returns']:\n    \"\"\"Classifies incoming customer intent\"\"\"\n\ncustomer_intent(\"I need to pick up my prescription\") # \"Pharmacy\"\n</code></pre> Notice <code>customer_intent</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin import ai_fn\n\n@ai_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"Generates a list of {{ n }} {{ color }} fruits\"\"\"\n\nlist_fruits(3) # \"['Apple', 'Cherry', 'Strawberry']\"\n</code></pre> Notice <code>list_fruits</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>Learning Marvin</p> <p>If you know Python, you already know Marvin. There are no fancy abstractions, just a handful of low-level, customizable decorators  to give your existing code superpowers and a number of utilities that make your life as an AI Engineer easier no matter what framework you use. </p> Sections Description Configuration Details on setting up Marvin and configuring various aspects of its behavior AI Components Documentation for Marvin's familiar, Pythonic interfaces to AI-powered functionality. API Utilities Low level API for building prompts and calling LLMs Examples Deeper dives into how to use Marvin"},{"location":"welcome/quickstart/","title":"Quickstart","text":"<p>After installing Marvin, the fastest way to get started is by using one of Marvin's high-level AI components. These components are designed to integrate AI into abstractions you already know well, creating the best possible opt-in developer experience.</p> <p>Initializing a Client</p> <p>To use Marvin you must have an API Key configured for an external model provider, like OpenAI. </p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n</code></pre>"},{"location":"welcome/quickstart/#components","title":"Components","text":""},{"location":"welcome/quickstart/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, built on Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data and entity extraction.</p> <p>Example</p> As a decoratorAs a function <p><code>ai_model</code> can decorate pydantic models to give them parsing powers. <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\n@ai_model(client = client)\nclass Location(BaseModel):\n    city: str\n    state_abbreviation: str = Field(\n        ..., \n        description=\"The two-letter state abbreviation\"\n    )\n\n\nLocation(\"The Big Apple\")\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>Location.as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code> {\n    \"tools\": [\n        {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"FormatResponse\",\n            \"parameters\": {\n            \"$defs\": {\n                \"Location\": {\n                \"properties\": {\n                    \"city\": {\n                    \"title\": \"City\",\n                    \"type\": \"string\"\n                    },\n                    \"state_abbreviation\": {\n                    \"description\": \"The two-letter state abbreviation\",\n                    \"title\": \"State Abbreviation\",\n                    \"type\": \"string\"\n                    }\n                },\n                \"required\": [\n                    \"city\",\n                    \"state_abbreviation\"\n                ],\n                \"title\": \"Location\",\n                \"type\": \"object\"\n                }\n            },\n            \"properties\": {\n                \"data\": {\n                \"allOf\": [\n                    {\n                    \"$ref\": \"#/$defs/Location\"\n                    }\n                ],\n                \"description\": \"The data to format.\"\n                }\n            },\n            \"required\": [\n                \"data\"\n            ],\n            \"type\": \"object\"\n            }\n        }\n        }\n    ],\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\n        \"name\": \"FormatResponse\"\n        }\n    },\n    \"messages\": [\n        {\n        \"content\": \"The user will provide context as text that you need to parse\n        into a structured form. To validate your response, you must call the \n        `FormatResponse` function. Use the provided text to extract or infer any \n        parameters needed by `FormatResponse`, including any missing data.\",\n        \"role\": \"system\"\n        },\n        {\n        \"content\": \"The text to parse: The Big Apple\",\n        \"role\": \"user\"\n        }\n    ]\n}\n</code></pre> <p><code>ai_model</code> can cast unstructured data to any <code>type</code> (or <code>GenericAlias</code>). <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\nclass Location(BaseModel):\n    city: str\n    state_abbreviation: str = Field(\n        ..., \n        description=\"The two-letter state abbreviation\"\n    )\n\nai_model(Location, client = client)(\"The Big Apple\")\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>ai_model(Location, client = client)(\"The Big Apple\").as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>    {\n    \"tools\": [\n        {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"FormatResponse\",\n            \"parameters\": {\n            \"$defs\": {\n                \"Location\": {\n                \"properties\": {\n                    \"city\": {\n                    \"title\": \"City\",\n                    \"type\": \"string\"\n                    },\n                    \"state_abbreviation\": {\n                    \"description\": \"The two-letter state abbreviation\",\n                    \"title\": \"State Abbreviation\",\n                    \"type\": \"string\"\n                    }\n                },\n                \"required\": [\n                    \"city\",\n                    \"state_abbreviation\"\n                ],\n                \"title\": \"Location\",\n                \"type\": \"object\"\n                }\n            },\n            \"properties\": {\n                \"data\": {\n                \"allOf\": [\n                    {\n                    \"$ref\": \"#/$defs/Location\"\n                    }\n                ],\n                \"description\": \"The data to format.\"\n                }\n            },\n            \"required\": [\n                \"data\"\n            ],\n            \"type\": \"object\"\n            }\n        }\n        }\n    ],\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\n        \"name\": \"FormatResponse\"\n        }\n    },\n    \"messages\": [\n        {\n        \"content\": \"The user will provide context as text that you need to parse\n        into a structured form. To validate your response, you must call the \n        `FormatResponse` function. Use the provided text to extract or infer any \n        parameters needed by `FormatResponse`, including any missing data.\",\n        \"role\": \"system\"\n        },\n        {\n        \"content\": \"The text to parse: The Big Apple\",\n        \"role\": \"user\"\n        }\n    ]\n}\n</code></pre> <p>Result</p> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"welcome/quickstart/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. Given user input, each classifier uses a clever logit bias trick to force an LLM to deductively choose the best option. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <p>Example</p> As a decoratorAs a function <p><code>ai_classifier</code> can decorate python functions whose return annotation is an <code>Enum</code> or <code>Literal</code>. The prompt is tuned for classification tasks,  and uses a form of <code>constrained sampling</code> to make guarantee a fast valid choice. <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n@ai_classifier(client = client)\ndef classify_intent(text: str) -&gt; AppRoute:\n    '''Classifies user's intent into most useful route'''\n\nclassify_intent(\"update my name\")\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>classify_intent.as_prompt(\"update my name\").serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. The prompt you send is fully customizable.  <pre><code>{\n\"logit_bias\": {\n    \"15\": 100.0,\n    \"16\": 100.0,\n    \"17\": 100.0,\n    \"18\": 100.0,\n    \"19\": 100.0,\n    \"20\": 100.0,\n    \"21\": 100.0,\n    \"22\": 100.0,\n    \"23\": 100.0\n},\n\"max_tokens\": 1,\n\"messages\": [\n    {\n    \"content\": \"## Expert Classifier\\n\\n        **Objective**: You are an expert classifier that always chooses correctly.\\n\\n        ### Context\\n        Classifies user's intent into most useful route\\n        \\n        ### Response Format\\n        You must classify the user provided data into one of the following classes:\\n        - Class 0 (value: USER_PROFILE)\\n        - Class 1 (value: SEARCH)\\n        - Class 2 (value: NOTIFICATIONS)\\n        - Class 3 (value: SETTINGS)\\n        - Class 4 (value: HELP)\\n        - Class 5 (value: CHAT)\\n        - Class 6 (value: DOCS)\\n        - Class 7 (value: PROJECTS)\\n        - Class 8 (value: WORKSPACES)\",\n    \"role\": \"system\"\n    },\n    {\n    \"content\": \"### Data\\n        The user provided the following data:                                                                                                                     \\n        - text: update my name\",\n    \"role\": \"assistant\"\n    },\n    {\n    \"content\": \"The most likely class label for the data and context provided above is Class\\\"\",\n    \"role\": \"assistant\"\n    }\n],\n\"temperature\": 0.0\n}\n</code></pre></p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\ndef classify_intent(text: str) -&gt; AppRoute:\n    '''Classifies user's intent into most useful route'''\n\nai_classifier(classify_intent, client = client)(\"update my name\")\n</code></pre> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>ai_classifier(classify_intent, client = client).as_prompt(\"update my name\").serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. The prompt you send is fully customizable.  <pre><code>{\n    \"logit_bias\": {\n        \"15\": 100.0,\n        \"16\": 100.0,\n        \"17\": 100.0,\n        \"18\": 100.0,\n        \"19\": 100.0,\n        \"20\": 100.0,\n        \"21\": 100.0,\n        \"22\": 100.0,\n        \"23\": 100.0\n    },\n    \"max_tokens\": 1,\n    \"messages\": [\n        {\n        \"content\": \"## Expert Classifier\\n\\n        **Objective**: You are an expert classifier that always chooses correctly.\\n\\n        ### Context\\n        Classifies user's intent into most useful route\\n        \\n        ### Response Format\\n        You must classify the user provided data into one of the following classes:\\n        - Class 0 (value: USER_PROFILE)\\n        - Class 1 (value: SEARCH)\\n        - Class 2 (value: NOTIFICATIONS)\\n        - Class 3 (value: SETTINGS)\\n        - Class 4 (value: HELP)\\n        - Class 5 (value: CHAT)\\n        - Class 6 (value: DOCS)\\n        - Class 7 (value: PROJECTS)\\n        - Class 8 (value: WORKSPACES)\",\n        \"role\": \"system\"\n        },\n        {\n        \"content\": \"### Data\\n        The user provided the following data:                                                                                                                     \\n        - text: update my name\",\n        \"role\": \"assistant\"\n        },\n        {\n        \"content\": \"The most likely class label for the data and context provided above is Class\\\"\",\n        \"role\": \"assistant\"\n        }\n    ],\n    \"temperature\": 0.0\n}\n</code></pre></p> <p>Result</p> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"welcome/quickstart/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis.</p> <p>Example</p> As a decoratorAs a function <p><code>ai_fn</code> can decorate python functions to evlaute them using a Large Language Model. <pre><code>from marvin import ai_fn\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\n@ai_fn(client=client)\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>sentiment_list.as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>{\n\"tools\": [\n    {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"FormatResponse\",\n        \"parameters\": {\n        \"properties\": {\n            \"data\": {\n            \"description\": \"The data to format.\",\n            \"items\": {\n                \"type\": \"number\"\n            },\n            \"title\": \"Data\",\n            \"type\": \"array\"\n            }\n        },\n        \"required\": [\n            \"data\"\n        ],\n        \"type\": \"object\"\n        }\n    }\n    }\n],\n\"tool_choice\": {\n    \"type\": \"function\",\n    \"function\": {\n    \"name\": \"FormatResponse\"\n    }\n},\n\"messages\": [\n    {\n    \"content\": \"Your job is to generate likely outputs for a Python function with the\\n        following signature and docstring:\\n\\n        \\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\\n    \\\"\\\"\\\"\\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\\n    -1 (negative) indicating their respective sentiment scores.\\n    \\\"\\\"\\\"\\n\\n\\n        The user will provide function inputs (if any) and you must respond with\\n        the most likely result.\",\n    \"role\": \"system\"\n    },\n    {\n    \"content\": \"The function was called with the following inputs:\\n        - texts: ['That was surprisingly easy!', 'Oh no, not again.']\\n\\n        What is its output?\",\n    \"role\": \"user\"\n    }\n]\n}\n</code></pre> <p><code>ai_fn</code> can be used as a utility function to evaluate python functions using a Large Language Model. <pre><code>from marvin import ai_fn\nfrom openai import OpenAI\n\nclient = OpenAI(api_key = 'YOUR_API_KEY')\n\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nai_fn(sentiment_list, client=client)(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre></p> Generated Prompt <p>You can view and/or eject the generated prompt by simply calling  <pre><code>ai_fn(sentiment_list, client = client)([\n    \"That was surprisingly easy!\",\n    \"Oh no, not again.\",\n]).as_prompt().serialize()\n</code></pre> When you do you'll see the raw payload that's sent to the LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>   {\n        \"tools\": [\n            {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"FormatResponse\",\n                \"parameters\": {\n                \"properties\": {\n                    \"data\": {\n                    \"description\": \"The data to format.\",\n                    \"items\": {\n                        \"type\": \"number\"\n                    },\n                    \"title\": \"Data\",\n                    \"type\": \"array\"\n                    }\n                },\n                \"required\": [\n                    \"data\"\n                ],\n                \"type\": \"object\"\n                }\n            }\n            }\n        ],\n        \"tool_choice\": {\n            \"type\": \"function\",\n            \"function\": {\n            \"name\": \"FormatResponse\"\n            }\n        },\n        \"messages\": [\n            {\n            \"content\": \"Your job is to generate likely outputs for a Python function with the\\n        following signature and docstring:\\n\\n        \\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\\n    \\\"\\\"\\\"\\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\\n    -1 (negative) indicating their respective sentiment scores.\\n    \\\"\\\"\\\"\\n\\n\\n        The user will provide function inputs (if any) and you must respond with\\n        the most likely result.\",\n            \"role\": \"system\"\n            },\n            {\n            \"content\": \"The function was called with the following inputs:\\n        - texts: ['That was surprisingly easy!', 'Oh no, not again.']\\n\\n        What is its output?\",\n            \"role\": \"user\"\n            }\n        ]\n        }\n</code></pre> <p>Result</p> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"welcome/quickstart/#utilities","title":"Utilities","text":"<p>Every Marvin component makes use of two serialization conveniences, which you're free to use  if you want to create your own opinionated components.</p>"},{"location":"welcome/quickstart/#prompt-functions","title":"Prompt Functions","text":"<p>Prompt Functions are responsible for taking a Python function and serializing it to a payload for a Large Language Model API to understand. It does not call or require an LLM provider. It's essentially a type-safe Jinja template that makes the locals of your function available for template formatting. </p> <p>Example</p> As a decoratorAs a function <p><code>prompt_fn</code> can decorate python functions to serialize them to a payload which them using a Large Language Model. It's especially useful if you want to use your own custom LLM but enjoy the ergonomics of Marvin. <pre><code>from marvin import prompt_fn\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"\n    Generates a list of {{ n }} {{ color }} fruits.\n    \"\"\"\n\n\nlist_fruits(3, 'blue')\n</code></pre></p> Result <p>It generates the raw payload that can be sent to an LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>{\n\"tools\": [\n    {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"FormatResponse\",\n        \"parameters\": {\n        \"properties\": {\n            \"data\": {\n            \"description\": \"The data to format.\",\n            \"items\": {\n                \"type\": \"string\"\n            },\n            \"title\": \"Data\",\n            \"type\": \"array\"\n            }\n        },\n        \"required\": [\n            \"data\"\n        ],\n        \"type\": \"object\"\n        }\n    }\n    }\n],\n\"tool_choice\": {\n    \"type\": \"function\",\n    \"function\": {\n    \"name\": \"FormatResponse\"\n    }\n},\n\"messages\": [\n    {\n    \"content\": \"Generate a list of 3 blue fruits.\",\n    \"role\": \"system\"\n    }\n]\n}\n</code></pre> <p><code>prompt_fn</code> can be used as a utility function to seraizlie python functions to prompts for a Large Language Model. It's especially useful if you want to use your own custom LLM but enjoy the ergonomics of Marvin. <pre><code>from marvin import prompt_fn\n\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"\n    Generates a list of {{ n }} {{ color }} fruits.\n    \"\"\"\n\n\nprompt_fn(list_fruits)(3, 'blue')\n</code></pre></p> Result <p>It generates the raw payload that can be sent to an LLM. All of the parameters below like <code>FormatResponse</code> and the prompt you send are fully customizable. </p> <pre><code>{\n\"tools\": [\n    {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"FormatResponse\",\n        \"parameters\": {\n        \"properties\": {\n            \"data\": {\n            \"description\": \"The data to format.\",\n            \"items\": {\n                \"type\": \"string\"\n            },\n            \"title\": \"Data\",\n            \"type\": \"array\"\n            }\n        },\n        \"required\": [\n            \"data\"\n        ],\n        \"type\": \"object\"\n        }\n    }\n    }\n],\n\"tool_choice\": {\n    \"type\": \"function\",\n    \"function\": {\n    \"name\": \"FormatResponse\"\n    }\n},\n\"messages\": [\n    {\n    \"content\": \"Generate a list of 3 blue fruits.\",\n    \"role\": \"system\"\n    }\n]\n}\n</code></pre>"},{"location":"welcome/quickstart/#response-models","title":"Response Models","text":"<p>For some applications, you may just want a helpful function calling utility instead of a full serialization layer.  You can use Marvin to wrap your client and enable it to handle a <code>response_model</code> keyword argument, or simply rely on our serialization / parsing primitives. Use whatever you need for your use case.</p>"},{"location":"welcome/quickstart/#marvinwrap","title":"Marvin.wrap","text":"<p>Under the hood, Marvin uses a convenience wrapper around the OpenAI API to pass Pydantic models as a <code>response_model</code> keyword. This let's you  use OpenAI to give you answers in a very specific way. </p> <p>Example</p> <pre><code>from marvin.client import Marvin\nfrom marvin.client.openai import MarvinClient\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = Marvin.wrap(OpenAI(api_key = 'YOUR_API_KEY'))\n\nclass Coffee(BaseModel):\n    '''A coffee order'''\n    size: str\n    with_milk: bool\n\nclient.chat.completions.create(\n    messages = [\n        {\n            'role': 'user', \n            'content': 'can I get a large latte?'\n        }\n    ], \n    response_model = Coffee\n)\n</code></pre> <p>Result</p> <pre><code>Coffee(size='large', with_milk=True)\n</code></pre>"},{"location":"welcome/quickstart/#pydantic","title":"Pydantic","text":"<p>The <code>Marvin.wrap</code> convenience is simply a wrapper around two utilities: <code>cast_model_to_toolset</code> and <code>cast_chat_completion_to_model</code>. As their names suggest each have a simple function: converting your pydantic models to a set of tools to be used by a large language model, and one for converting your response back into that model.</p> <p>Example</p> <p>We can recreate the example above in parts. First we'll spread <code>**</code> the toolset to our vanilla LLM call, and then parse its response. Everything here is  rigorously typed so it plays well with type hinters. <pre><code>from marvin._mappings.base_model import cast_model_to_toolset\nfrom marvin._mappings.chat_completion import cast_chat_completion_to_model\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclass Coffee(BaseModel):\n    '''A coffee order'''\n    size: str\n    with_milk: bool\n\nresponse = client.chat.completions.create(\n    model = 'gpt-3.5-turbo',\n    messages = [\n        {\n            'role': 'user', \n            'content': 'can I get a large latte?'\n        }\n    ], \n    **cast_model_to_toolset(Coffee).model_dump()\n)\n\ncast_chat_completion_to_model(Coffee, response)\n</code></pre></p> <p>Result</p> <pre><code>Coffee(size='large', with_milk=True)\n</code></pre>"},{"location":"welcome/what_is_marvin/","title":"What is Marvin?","text":"<p>Marvin is a Python library that lets you use Large Language Models by writing code, not prompts. It's open source, free to use, rigorously type-hinted, used by thousands of engineers, and built by the engineering team at Prefect.</p> Explain Like I'm Five I'm technicalI'm not technical <p>Marvin lets your software speak English and ask questions to LLMs.</p> <p>It introspects the types and docstrings of your functions and data models, and lets you cast them to prompts automatically to pass to a Large Language Model. This lets you write code as you normally would instead of writing prompts, and we handle the translation back and forth for you. </p> <p>This lets you focus on what you've always focused on: writing clean, versioned, reusable code and data models,  and not scrutinizing whether you begged your LLM hard enough to output JSON. </p> <p>Extracting, generating, cleaning, or classifying data is as simple as writing a function or a data model.</p> <p>Marvin lets engineers who know Python use Large Language Models without needing to write prompts.</p> <p>It turns out that ChatGPT and other Large Language Models are good at performing boring but incredibly valuable business-critical tasks beyond being a chatbot: you can use them to classify emails as spam, extract key figures from a report - exactly however you want for your scenario. When you use something like ChatGPT you spend a lot of time crafting the right prompt or context to get it to write your email, plan your date night, etc.</p> <p>If you want your software to use ChatGPT, you need to let it turn its objective into English. Marvin handles this 'translation' for you, so you get to just write code like you normally would. Engineers like using Marvin because it lets them write software like they're used to.</p> <p>Simply put, it lets you use Generative AI without feeling like you have to learn a framework.</p> <p>Marvin is lightweight and is built for incremental adoption. You can use it purely as a serialization library and bring your own stack, or fully use its engine to work with OpenAI and other providers. </p> <p>What Marvin feels like.</p> Structured Data ExtractionText ClassificationBusiness Logic <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin.components import ai_model\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n    latitude: float\n    longitude: float\n\nai_model(Location)(\"They say they're from the Windy City!\")\n# Location(city='Chicago', state='Illinois', latitude=41.8781, longitude=-87.6298)\n</code></pre> Notice there's no code written, just the expected types. Marvin's components turn your function into a prompt, uses AI to get its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin import ai_classifier\nfrom typing import Literal\n\n@ai_classifier\ndef customer_intent(text: str) -&gt; Literal['Store Hours', 'Pharmacy', 'Returns']:\n    \"\"\"Classifies incoming customer intent\"\"\"\n\ncustomer_intent(\"I need to pick up my prescription\") # \"Pharmacy\"\n</code></pre> Notice <code>customer_intent</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin import ai_fn\n\n@ai_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"Generates a list of {{ n }} {{ color }} fruits\"\"\"\n\nlist_fruits(3) # \"['Apple', 'Cherry', 'Strawberry']\"\n</code></pre> Notice <code>list_fruits</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>For years we've built open source software used by tens of thousands of data and machine learning engineers daily. Marvin brings those best practices for building dependable, observable software to generative AI. </p>"},{"location":"welcome/what_is_marvin/#what-models-do-we-support","title":"What models do we support?","text":"<p>Marvin supports any model so long as it adheres to the OpenAI spec. It's the easiest way to use Function Calling and Tool use. We run (and foot the bill for!) a public evaluation test suite to ensure that our library does what we say it does. If you're a community member who wants to build an maintain an integration with another provider, get in touch. </p> <p>Note that Marvin can be used as a serialization library, so you can bring your own Large Language Models and exclusively use Marvin to generate prompts from your code.</p>"},{"location":"welcome/what_is_marvin/#why-are-we-building-marvin","title":"Why are we building Marvin?","text":"<p>At Prefect we support thousands of engineers in workflow orchestration, from small startups to huge enterprise. In late 2022 we started working with our community to adopt AI into their workflows and found there wasn't a sane option for teams looking to build simple, quickly, and durable with Generative AI. </p>"},{"location":"welcome/what_is_marvin/#why-marvin-over-alternatives","title":"Why Marvin over alternatives?","text":"<p>Marvin's built and maintained by the team at Prefect. We work with thousands of engineers daily and work backwards from their  experiences to build reliable, intuitive and pleasant interfaces to otherwise hard things. </p> <p>There's a whole fleet of frameworks to work with Large Language Models, but we're not smart enough to understand them. We try to fight abstractions wherever we can so that users can easily understand and customize what's going on. </p>"}]}